{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NRMS Model (PyTorch Version)\n",
    "\n",
    "This notebook demonstrates how to build, train, and evaluate a Neural News Recommendation Model (NRMS) using PyTorch instead of TensorFlow. We will still attempt to use `ebrec` utilities for data loading and evaluation where possible.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We will:\n",
    "1.  Setup: Import necessary libraries and define hyperparameters.\n",
    "2.  Define NRMS Model Components: Implement custom layers and the NRMS model architecture.\n",
    "3.  Data Loading and Preparation: Load and preprocess the dataset.\n",
    "4.  Article Embeddings: Generate embeddings for articles using a pre-trained transformer model.\n",
    "5.  Batch and Shape Data: Create PyTorch datasets and dataloaders.\n",
    "6.  Training the Model: Train the NRMS model.\n",
    "7.  Evaluation on Test Set: Evaluate the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from ebrec.utils._behaviors import ebnerd_from_path, create_binary_labels_column, sampling_strategy_wu2019\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers, create_article_id_to_value_mapping\n",
    "from ebrec.utils._polars import concat_str_columns\n",
    "from ebrec.utils._constants import (\n",
    "\tDEFAULT_USER_COL, DEFAULT_IMPRESSION_ID_COL, DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "\tDEFAULT_HISTORY_ARTICLE_ID_COL, DEFAULT_CLICKED_ARTICLES_COL, DEFAULT_INVIEW_ARTICLES_COL\n",
    ")\n",
    "\n",
    "# Set random seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams:\n",
    "\ttitle_size = 30\n",
    "\thead_num = 16\n",
    "\thead_dim = 16\n",
    "\tattention_hidden_dim = 200\n",
    "\tdropout = 0.2\n",
    "\tbatch_size = 32\n",
    "\tverbose = False\n",
    "\tdata_fraction = 1 # Fraction of data to use\n",
    "\tsampling_nratio = 4 # For every positive sample ( a click ), we sample X negative samples\n",
    "\thistory_size = 20 # History of each users interactions will be limited to the most recent X articles\n",
    "\ttransformer_model_name = \"facebookai/xlm-roberta-base\"\n",
    "\n",
    "\tdef __str__(self):\n",
    "\t\treturn (\n",
    "\t\t\tf\"\\n title_size: {self.title_size}\"\n",
    "\t\t\tf\"\\n head_num: {self.head_num}\"\n",
    "\t\t\tf\"\\n head_dim: {self.head_dim}\"\n",
    "\t\t\tf\"\\n attention_hidden_dim: {self.attention_hidden_dim}\"\n",
    "\t\t\tf\"\\n dropout: {self.dropout}\"\n",
    "\t\t\tf\"\\n batch_size: {self.batch_size}\"\n",
    "\t\t\tf\"\\n verbose: {self.verbose}\"\n",
    "\t\t\tf\"\\n data_fraction: {self.data_fraction}\"\n",
    "\t\t\tf\"\\n sampling_nratio: {self.sampling_nratio}\"\n",
    "\t\t\tf\"\\n history_size: {self.history_size}\"\n",
    "\t\t\tf\"\\n transformer_model_name: {self.transformer_model_name}\"\n",
    "\t\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "\tdef __init__(self,hparams, verbose=False):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.head_num = hparams.head_num\n",
    "\t\tself.head_dim = hparams.head_dim\n",
    "\t\tself.output_dim = self.head_num * self.head_dim\n",
    "\t\tself.WQ = self.WK = self.WV = None\n",
    "\t\tself.dropout = nn.Dropout(hparams.dropout)\n",
    "\t\tself.verbose = verbose\n",
    "\n",
    "\tdef forward(self, Q_seq, K_seq, V_seq):\n",
    "\t\t# Lazy initialization of the weights\n",
    "\t\tif self.WQ is None:\n",
    "\t\t\tembedding_dim = Q_seq.size(-1)\n",
    "\t\t\tself.WQ = nn.Linear(embedding_dim, self.output_dim)\n",
    "\t\t\tself.WK = nn.Linear(embedding_dim, self.output_dim)\n",
    "\t\t\tself.WV = nn.Linear(embedding_dim, self.output_dim)\n",
    "\t\t\n",
    "\t\tQ = self.WQ(Q_seq)\n",
    "\t\tK = self.WK(K_seq)\n",
    "\t\tV = self.WV(V_seq)\n",
    "\n",
    "\t\tN, L, _ = Q.size()\n",
    "\t\tQ = Q.view(N, L, self.head_num, self.head_dim).transpose(1, 2)\n",
    "\t\tK = K.view(N, L, self.head_num, self.head_dim).transpose(1, 2)\n",
    "\t\tV = V.view(N, L, self.head_num, self.head_dim).transpose(1, 2)\n",
    "\n",
    "\t\tif self.verbose:\n",
    "\t\t\tprint(f\"Q shape: {Q.shape}\")\n",
    "\t\t\tprint(f\"K shape: {K.shape}\")\n",
    "\t\t\tprint(f\"V shape: {V.shape}\")\n",
    "\n",
    "\t\tscores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "\t\tattn = torch.softmax(scores, dim=-1)\n",
    "\t\tattn = self.dropout(attn)\n",
    "\n",
    "\t\toutput = torch.matmul(attn, V)\n",
    "\t\toutput = output.transpose(1, 2).contiguous().view(N, L, self.output_dim)\n",
    "\n",
    "\t\tif self.verbose:\n",
    "\t\t\tprint(f\"Attention shape: {attn.shape}\")\n",
    "\t\t\tprint(f\"Output shape: {output.shape}\")\n",
    "\n",
    "\t\treturn output\n",
    "\n",
    "class AttLayer(nn.Module):\n",
    "\tdef __init__(self, hparams, verbose=False):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.W = nn.Linear(hparams.head_num * hparams.head_dim, hparams.attention_hidden_dim)\n",
    "\t\tself.q = nn.Linear(hparams.attention_hidden_dim, 1, bias=False)\n",
    "\t\tself.dropout = nn.Dropout(hparams.dropout)\n",
    "\t\tself.verbose = verbose\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tattn = torch.tanh(self.W(x))\n",
    "\t\tattn = self.q(attn).squeeze(-1)\n",
    "\t\tattn = torch.softmax(attn, dim=1).unsqueeze(-1)\n",
    "\n",
    "\t\tif self.verbose:\n",
    "\t\t\tprint(f\"Attention weights shape: {attn.shape}\")\n",
    "\t\t\tprint(f\"Input shape: {x.shape}\")\n",
    "\n",
    "\t\toutput = torch.sum(x * attn, dim=1)\n",
    "\t\toutput = self.dropout(output)\n",
    "\n",
    "\t\tif self.verbose:\n",
    "\t\t\tprint(f\"Output shape: {output.shape}\")\n",
    "\n",
    "\t\treturn output\n",
    " \n",
    "class NRMSModel(nn.Module):\n",
    "\tdef __init__(self, hparams, word_embeddings):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.embedding = nn.Embedding.from_pretrained(\n",
    "\t\t\ttorch.FloatTensor(word_embeddings), freeze=False\n",
    "\t\t\t)\n",
    "\t\tself.dropout = nn.Dropout(hparams.dropout)\n",
    "\n",
    "\t\t# News Encoder\n",
    "\t\tself.news_self_att = SelfAttention(hparams, verbose=hparams.verbose)\n",
    "\t\tself.news_att = AttLayer(hparams, verbose=hparams.verbose)\n",
    "\n",
    "\t\t# User Encoder\n",
    "\t\tself.user_self_att = SelfAttention(hparams, verbose=hparams.verbose)\n",
    "\t\tself.user_att = AttLayer(hparams, verbose=hparams.verbose)\n",
    "\n",
    "\tdef encode_news(self, news_input):\n",
    "\t\tx = self.embedding(news_input)\n",
    "\t\tx = self.dropout(x)\n",
    "\t\tx = self.news_self_att(x, x, x)\n",
    "\t\tx = self.news_att(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef encode_user(self, history_input):\n",
    "\t\tN, H, L = history_input.size()\n",
    "\t\thistory_input = history_input.view(N * H, L)\n",
    "\t\tnews_vectors = self.encode_news(history_input)\n",
    "\t\tnews_vectors = news_vectors.view(N, H, -1)\n",
    "\t\tuser_vector = self.user_self_att(news_vectors, news_vectors, news_vectors)\n",
    "\t\tuser_vector = self.user_att(user_vector)\n",
    "\t\treturn user_vector\n",
    "\n",
    "\tdef forward(self, his_input, pred_input):\n",
    "\t\tuser_vector = self.encode_user(his_input)\n",
    "\t\tN, M, L = pred_input.size()\n",
    "\t\tpred_input = pred_input.view(N * M, L)\n",
    "\t\tnews_vectors = self.encode_news(pred_input)\n",
    "\t\tnews_vectors = news_vectors.view(N, M, -1)\n",
    "\t\tscores = torch.bmm(news_vectors, user_vector.unsqueeze(2)).squeeze(-1)\n",
    "\t\treturn scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NRMSDataset(Dataset):\n",
    "\tdef __init__(self, df, article_mapping, title_size, history_column, candidate_column, verbose=False):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tdf (pl.DataFrame): DataFrame containing raw history and candidate article IDs.\n",
    "\t\t\tarticle_mapping (dict): Mapping of article IDs to tokenized representations.\n",
    "\t\t\ttitle_size (int): Maximum size of title tokens for padding/truncation.\n",
    "\t\t\thistory_column (str): Column containing user history.\n",
    "\t\t\tcandidate_column (str): Column containing candidate articles.\n",
    "\t\t\tverbose (bool): If True, prints debug information.\n",
    "\t\t\"\"\"\n",
    "\t\tself.history_raw = df[history_column].to_list()\n",
    "\t\tself.candidates_raw = df[candidate_column].to_list()\n",
    "\t\tself.labels = df[\"labels\"].to_list()\n",
    "\t\tself.article_mapping = article_mapping\n",
    "\t\tself.title_size = title_size\n",
    "\t\tself.verbose = verbose\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.labels)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t# Convert article IDs to tokenized representations\n",
    "\t\thistory_tokens = [\n",
    "\t\t\tself.article_mapping.get(aid, [0] * self.title_size) for aid in self.history_raw[idx]\n",
    "\t\t]\n",
    "\t\tcandidate_tokens = [\n",
    "\t\t\tself.article_mapping.get(aid, [0] * self.title_size) for aid in self.candidates_raw[idx]\n",
    "\t\t]\n",
    "\t\t\n",
    "\t\t# Convert to PyTorch tensors\n",
    "\t\this_ids = torch.tensor(history_tokens, dtype=torch.long)\n",
    "\t\tpred_ids = torch.tensor(candidate_tokens, dtype=torch.long)\n",
    "\t\ty = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "\t\tif self.verbose:\n",
    "\t\t\tprint(f\"History Tokens: {his_ids.shape}\")\n",
    "\t\t\tprint(f\"Candidate Tokens: {pred_ids.shape}\")\n",
    "\t\t\tprint(f\"Label: {y.shape}\")\n",
    "\n",
    "\t\treturn his_ids, pred_ids, y\n",
    "\n",
    "def nrms_collate_fn(batch):\n",
    "\thistories, candidates, labels = zip(*batch)\n",
    "\tmax_candidates = max([cand.size(0) for cand in candidates])\n",
    "\t\n",
    "\tpadded_candidates = []\n",
    "\tcandidate_masks = []\n",
    "\tfor cand in candidates:\n",
    "\t\tnum_cands = cand.size(0)\n",
    "\t\tif num_cands < max_candidates:\n",
    "\t\t\tpad_size = max_candidates - num_cands\n",
    "\t\t\tpadded_cand = torch.cat([cand, torch.zeros(pad_size, cand.size(1), dtype=torch.long)])\n",
    "\t\t\tmask = torch.cat([torch.ones(num_cands, dtype=torch.bool), torch.zeros(pad_size, dtype=torch.bool)])\n",
    "\t\telse:\n",
    "\t\t\tpadded_cand = cand[:max_candidates]\n",
    "\t\t\tmask = torch.ones(max_candidates, dtype=torch.bool)\n",
    "\t\tpadded_candidates.append(padded_cand)\n",
    "\t\tcandidate_masks.append(mask)\n",
    "\t\n",
    "\tpadded_candidates = torch.stack(padded_candidates)\n",
    "\tcandidate_masks = torch.stack(candidate_masks)\n",
    "\thistories = torch.stack(histories)\n",
    "\t\n",
    "\tpadded_labels = []\n",
    "\tfor label in labels:\n",
    "\t\tnum_cands = label.size(0)\n",
    "\t\tif num_cands < max_candidates:\n",
    "\t\t\tpad_size = max_candidates - num_cands\n",
    "\t\t\tpadded_label = torch.cat([label, torch.zeros(pad_size, dtype=torch.float32)])\n",
    "\t\telse:\n",
    "\t\t\tpadded_label = label[:max_candidates]\n",
    "\t\tpadded_labels.append(padded_label)\n",
    "\tpadded_labels = torch.stack(padded_labels)\n",
    "\t\n",
    "\treturn {\n",
    "\t\t'history': histories,\n",
    "\t\t'candidates': padded_candidates,\n",
    "\t\t'labels': padded_labels,\n",
    "\t\t'candidate_masks': candidate_masks\n",
    "\t}\n",
    "\n",
    "def create_dataloader(df, article_mapping, title_size, batch_size, history_column, candidate_column, shuffle=False):\n",
    "\tdataset = NRMSDataset(\n",
    "\t\tdf, article_mapping, title_size, history_column, candidate_column\n",
    "\t)\n",
    "\treturn DataLoader(\n",
    "\t\tdataset,\n",
    "\t\tbatch_size=batch_size,\n",
    "\t\tshuffle=shuffle,\n",
    "\t\tcollate_fn=nrms_collate_fn,\n",
    "\t\tnum_workers=0,  # Adjust based on your system\n",
    "\t)\n",
    "\n",
    "def prepare_df_for_training(df):\n",
    "\t\"\"\"\n",
    "\tValidate and preprocess DataFrame to ensure required columns exist.\n",
    "\t\"\"\"\n",
    "\trequired_columns = [DEFAULT_HISTORY_ARTICLE_ID_COL, DEFAULT_INVIEW_ARTICLES_COL, \"labels\"]\n",
    "\tfor col in required_columns:\n",
    "\t\tif col not in df.columns:\n",
    "\t\t\traise ValueError(f\"Missing required column: {col}\")\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Helper Function\n",
    "1. Optimizer: Adam\n",
    "2. Loss Function: Cross Entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(device, model, train_loader, val_loader, num_epochs, learning_rate=1e-3, patience=3):\n",
    "\tmodel = model.to(device)\n",
    "\n",
    "\tcriterion = nn.CrossEntropyLoss()\n",
    "\toptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\tbest_val_loss = float('inf')\n",
    "\tbest_val_auc = 0\n",
    "\tpatience_counter = 0\n",
    "\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\t# Training\n",
    "\t\tmodel.train()\n",
    "\t\ttotal_loss = 0\n",
    "\t\twith tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "\t\t\tfor batch in train_loader:\n",
    "\t\t\t\this_input = batch['history'].to(device)\n",
    "\t\t\t\tpred_input = batch['candidates'].to(device)\n",
    "\t\t\t\tlabels = batch['labels'].to(device)\n",
    "\t\t\t\tmasks = batch['candidate_masks'].to(device)\n",
    "\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\tscores = model(his_input, pred_input)\n",
    "\t\t\t\tscores = scores * masks\n",
    "\t\t\t\tloss = criterion(scores, labels)\n",
    "\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\ttotal_loss += loss.item()\n",
    "\t\t\t\tpbar.update(1)\n",
    "\n",
    "\t\tavg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "\t\t# Validation\n",
    "\t\tmodel.eval()\n",
    "\t\tval_loss = 0\n",
    "\t\tall_scores = []\n",
    "\t\tall_labels = []\n",
    "\t\twith torch.no_grad(), tqdm(total=len(val_loader), desc=\"Validation\", unit=\"batch\") as pbar:\n",
    "\t\t\tfor batch in val_loader:\n",
    "\t\t\t\this_input = batch['history'].to(device)\n",
    "\t\t\t\tpred_input = batch['candidates'].to(device)\n",
    "\t\t\t\tlabels = batch['labels']\n",
    "\t\t\t\tmasks = batch['candidate_masks']\n",
    "\n",
    "\t\t\t\tscores = model(his_input, pred_input)\n",
    "\t\t\t\tscores = scores * masks\n",
    "\t\t\t\tloss = criterion(scores, labels.to(device))\n",
    "\t\t\t\tval_loss += loss.item()\n",
    "\n",
    "\t\t\t\tvalid_scores = scores[masks.bool()].cpu().numpy()\n",
    "\t\t\t\tvalid_labels = labels[masks.bool()].numpy()\n",
    "\t\t\t\tall_scores.extend(valid_scores)\n",
    "\t\t\t\tall_labels.extend(valid_labels)\n",
    "\t\t\t\tpbar.update(1)\n",
    "\n",
    "\t\tavg_val_loss = val_loss / len(val_loader)\n",
    "\t\tval_auc = roc_auc_score(all_labels, all_scores)\n",
    "\t\tauc_improvement = val_auc - best_val_auc if epoch > 0 else val_auc\n",
    "\t\tprint(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val AUC: {val_auc:.4f}, Improvement from Previous Epoch: {auc_improvement:.4f}\")\n",
    "\t\tbest_val_auc = max(best_val_auc, val_auc)\n",
    "\t\t\n",
    "\t\tcheckpoint_dir = \"checkpoints\"\n",
    "\t\tos.makedirs(checkpoint_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\t\tcheckpoint_path = f\"checkpoints/nrms_checkpoint_{epoch+1}.pth\"\n",
    "\t\ttorch.save({\n",
    "\t\t\t'epoch': epoch+1, \n",
    "\t\t\t'model_state_dict': model.state_dict(), \n",
    "\t\t\t'optimizer_state_dict': optimizer.state_dict(), \n",
    "\t\t\t'loss': avg_val_loss, \n",
    "\t\t\t'auc': val_auc}, checkpoint_path)\n",
    "\t\tprint(f\"Checkpoint saved to: {checkpoint_path}\")\n",
    "\n",
    "\n",
    "\t\t# Early stopping\n",
    "\t\tif avg_val_loss < best_val_loss:\n",
    "\t\t\tbest_val_loss = avg_val_loss\n",
    "\t\t\tpatience_counter = 0\n",
    "\t\telse:\n",
    "\t\t\tpatience_counter += 1\n",
    "\t\t\tif patience_counter >= patience:\n",
    "\t\t\t\tprint(\"Early stopping triggered\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: \n",
      " title_size: 30\n",
      " head_num: 16\n",
      " head_dim: 16\n",
      " attention_hidden_dim: 200\n",
      " dropout: 0.2\n",
      " batch_size: 32\n",
      " verbose: False\n",
      " data_fraction: 0.01\n",
      " sampling_nratio: 4\n",
      " history_size: 20\n",
      " transformer_model_name: facebookai/xlm-roberta-base\n"
     ]
    }
   ],
   "source": [
    "# Setting hyperparameters\n",
    "hparams=HParams()\n",
    "hparams.data_fraction = 0.01\n",
    "hparams.batch_size = 32\n",
    "print(\"Hyperparameters:\", hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 2016\n",
      "Validation samples: 326\n"
     ]
    }
   ],
   "source": [
    "# Data loading\n",
    "PATH = Path(\"~/Git Repositories/ebnerd-benchmark/data\").expanduser()\n",
    "DATASPLIT = \"ebnerd_small\"\n",
    "\n",
    "# Load and process training data\n",
    "df_train = (\n",
    "\tebnerd_from_path(\n",
    "\t\tPATH.joinpath(DATASPLIT, \"train\"),\n",
    "\t\thistory_size=hparams.history_size,\n",
    "\t\tpadding=0,\n",
    "\t)\n",
    "\t.pipe(\n",
    "\t\tsampling_strategy_wu2019,\n",
    "\t\tnpratio=hparams.sampling_nratio,\n",
    "\t\twith_replacement=True,\n",
    "\t\tseed=seed,\n",
    "\t)\n",
    "\t.pipe(create_binary_labels_column)\n",
    "\t.sample(fraction=hparams.data_fraction)\n",
    ")\n",
    "\n",
    "\n",
    "# Split into train/validation\n",
    "dt_split = df_train[DEFAULT_IMPRESSION_TIMESTAMP_COL].max() - datetime.timedelta(days=1)\n",
    "df_train_split = df_train.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL) < dt_split)\n",
    "df_validation = df_train.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL) >= dt_split)\n",
    "print(f\"Train samples: {df_train_split.height}\\nValidation samples: {df_validation.height}\")\n",
    "\n",
    "# Load articles and prepare embeddings\n",
    "df_articles = pl.read_parquet(PATH.joinpath(\"articles.parquet\"))\n",
    "transformer_model = AutoModel.from_pretrained(hparams.transformer_model_name)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(hparams.transformer_model_name)\n",
    "word_embeddings = transformer_model.get_input_embeddings().weight.detach().numpy()\n",
    "\n",
    "# Prepare article embeddings\n",
    "df_articles, cat_col = concat_str_columns(df_articles, columns=[\"subtitle\", \"title\"])\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "\tdf_articles, \n",
    "\ttransformer_tokenizer, \n",
    "\tcat_col, \n",
    "\tmax_length=hparams.title_size\n",
    ")\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "\tdf=df_articles, \n",
    "\tvalue_col=token_col_title\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with  \n",
      " title_size: 30\n",
      " head_num: 16\n",
      " head_dim: 16\n",
      " attention_hidden_dim: 200\n",
      " dropout: 0.2\n",
      " batch_size: 32\n",
      " verbose: False\n",
      " data_fraction: 0.01\n",
      " sampling_nratio: 4\n",
      " history_size: 20\n",
      " transformer_model_name: facebookai/xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:  10%|▉         | 6/63 [00:06<00:57,  1.01s/batch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m NRMSModel(hparams, word_embeddings)\n\u001b[1;32m     27\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 29\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m\t\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\n\u001b[1;32m     37\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[50], line 27\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(device, model, train_loader, val_loader, num_epochs, learning_rate, patience)\u001b[0m\n\u001b[1;32m     24\u001b[0m scores \u001b[38;5;241m=\u001b[39m scores \u001b[38;5;241m*\u001b[39m masks\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(scores, labels)\n\u001b[0;32m---> 27\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     29\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/anaconda3/envs/eb-nerd/lib/python3.11/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/eb-nerd/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/eb-nerd/lib/python3.11/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Validate DataFrames\n",
    "df_train_split = prepare_df_for_training(df_train_split)\n",
    "df_validation = prepare_df_for_training(df_validation)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = create_dataloader(\n",
    "\tdf_train_split, article_mapping, hparams.title_size,\n",
    "\tbatch_size=hparams.batch_size,\n",
    "\thistory_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "\tcandidate_column=DEFAULT_INVIEW_ARTICLES_COL,\n",
    "\tshuffle=True\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader(\n",
    "\tdf_validation, article_mapping, hparams.title_size,\n",
    "\tbatch_size=hparams.batch_size,\n",
    "\thistory_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "\tcandidate_column=DEFAULT_INVIEW_ARTICLES_COL,\n",
    "\tshuffle=False\n",
    ")\n",
    "\n",
    "# Initialize and train model\n",
    "print(\"Training model with \", hparams)\n",
    "device = torch.device('cunda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NRMSModel(hparams, word_embeddings)\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "model = train_and_evaluate(\n",
    "  device,\n",
    "\tmodel, \n",
    "\ttrain_loader, \n",
    "\tval_loader, \n",
    "\tnum_epochs=EPOCHS, \n",
    "\tlearning_rate=1e-3, \n",
    "\tpatience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "auc: 0.5555\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "\tmodel.eval()\n",
    "\tall_scores = []\n",
    "\tall_labels = []\n",
    "\twith torch.no_grad():\n",
    "\t\tfor batch in dataloader:\n",
    "\t\t\this_input = batch['history'].to(device)\n",
    "\t\t\tpred_input = batch['candidates'].to(device)\n",
    "\t\t\tlabels = batch['labels']\n",
    "\t\t\tmasks = batch['candidate_masks']\n",
    "\n",
    "\t\t\tscores = model(his_input, pred_input)\n",
    "\t\t\tvalid_scores = scores[masks.bool()].cpu().numpy()\n",
    "\t\t\tvalid_labels = labels[masks.bool()].numpy()\n",
    "\t\t\tall_scores.extend(valid_scores)\n",
    "\t\t\tall_labels.extend(valid_labels)\n",
    "\n",
    "\tall_scores = np.array(all_scores)\n",
    "\tall_labels = np.array(all_labels)\n",
    "\n",
    "\tmetrics = {\n",
    "\t\t'auc': roc_auc_score(all_labels, all_scores),\n",
    "\t}\n",
    "\treturn metrics\n",
    "\n",
    "# Evaluate model\n",
    "metrics = evaluate_model(model, val_loader, device)\n",
    "print(\"\\nValidation Metrics:\")\n",
    "for metric_name, value in metrics.items():\n",
    "\tprint(f\"{metric_name}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eb-nerd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
