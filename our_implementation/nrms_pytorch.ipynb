{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yg/512j_wjj4_vf349wh5rh4xlc0000gn/T/ipykernel_84605/2760744591.py:232: DeprecationWarning: `apply` is deprecated. It has been renamed to `map_elements`.\n",
      "  pl.col(DEFAULT_HISTORY_ARTICLE_ID_COL).apply(\n",
      "/var/folders/yg/512j_wjj4_vf349wh5rh4xlc0000gn/T/ipykernel_84605/2760744591.py:239: DeprecationWarning: `apply` is deprecated. It has been renamed to `map_elements`.\n",
      "  pl.col(DEFAULT_INVIEW_ARTICLES_COL).apply(\n",
      "/var/folders/yg/512j_wjj4_vf349wh5rh4xlc0000gn/T/ipykernel_84605/2760744591.py:232: DeprecationWarning: `apply` is deprecated. It has been renamed to `map_elements`.\n",
      "  pl.col(DEFAULT_HISTORY_ARTICLE_ID_COL).apply(\n",
      "/var/folders/yg/512j_wjj4_vf349wh5rh4xlc0000gn/T/ipykernel_84605/2760744591.py:239: DeprecationWarning: `apply` is deprecated. It has been renamed to `map_elements`.\n",
      "  pl.col(DEFAULT_INVIEW_ARTICLES_COL).apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 1.6124\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from ebrec.utils._behaviors import (\n",
    "    ebnerd_from_path,\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    ")\n",
    "from ebrec.utils._articles import (\n",
    "    convert_text2encoding_with_transformers,\n",
    "    create_article_id_to_value_mapping,\n",
    ")\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "from ebrec.utils._polars import concat_str_columns\n",
    "from ebrec.utils._constants import *\n",
    "\n",
    "# Set random seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Hyperparameters\n",
    "class HParams:\n",
    "    title_size = 30\n",
    "    history_size = 20\n",
    "    head_num = 16\n",
    "    head_dim = 16\n",
    "    attention_hidden_dim = 200\n",
    "    dropout = 0.2\n",
    "    learning_rate = 1e-4\n",
    "\n",
    "hparams = HParams()\n",
    "\n",
    "# Transformer model name\n",
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "\n",
    "# Data paths\n",
    "PATH = Path(\"~/Git Repositories/ebnerd-benchmark/data\").expanduser()\n",
    "DATASPLIT = \"ebnerd_small\"\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "]\n",
    "\n",
    "# Load and preprocess data\n",
    "FRACTION = 0.01\n",
    "df = (\n",
    "    ebnerd_from_path(\n",
    "        PATH.joinpath(DATASPLIT, \"train\"),\n",
    "        history_size=hparams.history_size,\n",
    "        padding=0,\n",
    "    )\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=4,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "\n",
    "dt_split = df[DEFAULT_IMPRESSION_TIMESTAMP_COL].max() - datetime.timedelta(days=1)\n",
    "df_train = df.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL) < dt_split)\n",
    "df_validation = df.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL) >= dt_split)\n",
    "\n",
    "# Load articles data\n",
    "df_articles = pl.read_parquet(PATH.joinpath(DATASPLIT, \"articles.parquet\"))\n",
    "\n",
    "# Load transformer model and tokenizer\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_model.eval()\n",
    "\n",
    "# Get word embeddings\n",
    "word_embeddings = transformer_model.get_input_embeddings().weight.detach().numpy()\n",
    "\n",
    "# Prepare article mappings\n",
    "df_articles, cat_col = concat_str_columns(df_articles, columns=[\"subtitle\", \"title\"])\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_col, max_length=hparams.title_size\n",
    ")\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")\n",
    "\n",
    "# Fix article mapping values if necessary\n",
    "for k, v in article_mapping.items():\n",
    "    if isinstance(v, list) and len(v) > 0:\n",
    "        article_mapping[k] = v[0]\n",
    "    elif isinstance(v, list) and len(v) == 0:\n",
    "        article_mapping[k] = [0] * hparams.title_size\n",
    "\n",
    "# Define NRMS model components\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, head_num, head_dim):\n",
    "        super().__init__()\n",
    "        self.head_num = head_num\n",
    "        self.head_dim = head_dim\n",
    "        self.output_dim = head_num * head_dim\n",
    "        self.WQ = None\n",
    "        self.WK = None\n",
    "        self.WV = None\n",
    "        self.initialized = False\n",
    "\n",
    "    def _initialize(self, input_dim):\n",
    "        self.WQ = nn.Parameter(torch.empty(input_dim, self.output_dim))\n",
    "        self.WK = nn.Parameter(torch.empty(input_dim, self.output_dim))\n",
    "        self.WV = nn.Parameter(torch.empty(input_dim, self.output_dim))\n",
    "        nn.init.xavier_uniform_(self.WQ)\n",
    "        nn.init.xavier_uniform_(self.WK)\n",
    "        nn.init.xavier_uniform_(self.WV)\n",
    "        self.initialized = True\n",
    "\n",
    "    def forward(self, Q_seq, K_seq, V_seq):\n",
    "        if not self.initialized:\n",
    "            self._initialize(Q_seq.size(-1))\n",
    "\n",
    "        Q = torch.matmul(Q_seq, self.WQ)\n",
    "        K = torch.matmul(K_seq, self.WK)\n",
    "        V = torch.matmul(V_seq, self.WV)\n",
    "\n",
    "        N, L, _ = Q.size()\n",
    "        Q = Q.view(N, L, self.head_num, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(N, L, self.head_num, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(N, L, self.head_num, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        A = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(self.head_dim)\n",
    "        A = torch.softmax(A, dim=-1)\n",
    "        O = torch.matmul(A, V)\n",
    "\n",
    "        O = O.permute(0, 2, 1, 3).contiguous().view(N, L, self.output_dim)\n",
    "        return O\n",
    "\n",
    "class AttLayer(nn.Module):\n",
    "    def __init__(self, attention_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attention_hidden_dim = attention_hidden_dim\n",
    "        self.W = None\n",
    "        self.q = None\n",
    "        self.initialized = False\n",
    "\n",
    "    def _initialize(self, input_dim):\n",
    "        self.W = nn.Linear(input_dim, self.attention_hidden_dim)\n",
    "        self.q = nn.Linear(self.attention_hidden_dim, 1, bias=False)\n",
    "        self.initialized = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.initialized:\n",
    "            self._initialize(x.size(-1))\n",
    "        attention = torch.tanh(self.W(x))\n",
    "        attention = self.q(attention).squeeze(-1)\n",
    "        att_weight = torch.softmax(attention, dim=1).unsqueeze(-1)\n",
    "        output = torch.sum(x * att_weight, dim=1)\n",
    "        return output\n",
    "\n",
    "class NRMSModel(nn.Module):\n",
    "    def __init__(self, hparams, word_embeddings):\n",
    "        super().__init__()\n",
    "        self.hparams = hparams\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(word_embeddings), freeze=False\n",
    "        )\n",
    "        self.dropout = nn.Dropout(hparams.dropout)\n",
    "\n",
    "        # News Encoder\n",
    "        self.news_self_att = SelfAttention(hparams.head_num, hparams.head_dim)\n",
    "        self.news_att = AttLayer(hparams.attention_hidden_dim)\n",
    "\n",
    "        # User Encoder\n",
    "        self.user_self_att = SelfAttention(hparams.head_num, hparams.head_dim)\n",
    "        self.user_att = AttLayer(hparams.attention_hidden_dim)\n",
    "\n",
    "    def encode_news(self, news_input):\n",
    "        x = self.embedding(news_input)\n",
    "        x = self.dropout(x)\n",
    "        x = self.news_self_att(x, x, x)\n",
    "        x = self.news_att(x)\n",
    "        return x\n",
    "\n",
    "    def encode_user(self, history_input):\n",
    "        N, H, L = history_input.size()\n",
    "        history_input = history_input.view(N * H, L)\n",
    "        news_vectors = self.encode_news(history_input)\n",
    "        news_vectors = news_vectors.view(N, H, -1)\n",
    "        user_vector = self.user_self_att(news_vectors, news_vectors, news_vectors)\n",
    "        user_vector = self.user_att(user_vector)\n",
    "        return user_vector\n",
    "\n",
    "    def forward(self, his_input, pred_input):\n",
    "        user_vector = self.encode_user(his_input)           # Shape: [N, D]\n",
    "        N, M, L = pred_input.size()\n",
    "        pred_input = pred_input.view(N * M, L)\n",
    "        news_vectors = self.encode_news(pred_input)         # Shape: [N*M, D]\n",
    "        news_vectors = news_vectors.view(N, M, -1)          # Shape: [N, M, D]\n",
    "        user_vector = user_vector.unsqueeze(2)              # Shape: [N, D, 1]\n",
    "        scores = torch.bmm(news_vectors, user_vector).squeeze(-1)  # Shape: [N, M]\n",
    "        return scores\n",
    "\n",
    "# Dataset Class\n",
    "class NRMSDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.history = df[\"history_tokens\"].to_list()\n",
    "        self.candidates = df[\"candidate_tokens\"].to_list()\n",
    "        self.labels = df[\"labels\"].to_list()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        his_ids = torch.tensor(self.history[idx], dtype=torch.long)\n",
    "        pred_ids = torch.tensor(self.candidates[idx], dtype=torch.long)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return his_ids, pred_ids, y\n",
    "\n",
    "# Transform dataframes\n",
    "def transform_df(df, article_mapping):\n",
    "    df = df.with_columns(\n",
    "        pl.col(DEFAULT_HISTORY_ARTICLE_ID_COL).apply(\n",
    "            lambda history: [\n",
    "                article_mapping.get(aid, [0] * hparams.title_size) for aid in history\n",
    "            ]\n",
    "        ).alias(\"history_tokens\")\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "        pl.col(DEFAULT_INVIEW_ARTICLES_COL).apply(\n",
    "            lambda candidates: [\n",
    "                article_mapping.get(aid, [0] * hparams.title_size) for aid in candidates\n",
    "            ]\n",
    "        ).alias(\"candidate_tokens\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Transform training and validation data\n",
    "df_train = transform_df(df_train, article_mapping)\n",
    "df_validation = transform_df(df_validation, article_mapping)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = NRMSDataset(df_train)\n",
    "val_dataset = NRMSDataset(df_validation)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "model = NRMSModel(hparams, word_embeddings)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, hparams, num_epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=hparams.learning_rate)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for his_input, pred_input, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            scores = model(his_input, pred_input)\n",
    "            targets = torch.argmax(labels, dim=1)\n",
    "            loss = criterion(scores, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 1\n",
    "train_model(model, train_loader, hparams, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eb-nerd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
