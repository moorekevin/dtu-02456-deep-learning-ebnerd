{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NRMS Model (PyTorch Version)\n",
    "\n",
    "This notebook demonstrates how to build, train, and evaluate a Neural News Recommendation Model (NRMS) using PyTorch instead of TensorFlow. We will still attempt to use `ebrec` utilities for data loading and evaluation where possible.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We will:\n",
    "1.  Setup: Import necessary libraries and define hyperparameters.\n",
    "2.  Define NRMS Model Components: Implement custom layers and the NRMS model architecture.\n",
    "3.  Data Loading and Preparation: Load and preprocess the dataset.\n",
    "4.  Article Embeddings: Generate embeddings for articles using a pre-trained transformer model.\n",
    "5.  Batch and Shape Data: Create PyTorch datasets and dataloaders.\n",
    "6.  Training the Model: Train the NRMS model.\n",
    "7.  Evaluation on Test Set: Evaluate the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinmoore/anaconda3/envs/eb-nerd/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from ebrec.utils._behaviors import ebnerd_from_path, create_binary_labels_column, sampling_strategy_wu2019\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers, create_article_id_to_value_mapping\n",
    "from ebrec.utils._polars import concat_str_columns\n",
    "from ebrec.utils._constants import (\n",
    "\tDEFAULT_USER_COL, DEFAULT_IMPRESSION_ID_COL, DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "\tDEFAULT_HISTORY_ARTICLE_ID_COL, DEFAULT_CLICKED_ARTICLES_COL, DEFAULT_INVIEW_ARTICLES_COL\n",
    ")\n",
    "from ebrec.evaluation.metrics._ranking import ndcg_score, mrr_score\n",
    "\n",
    "# Set random seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams:\n",
    "\ttitle_size = 30\n",
    "\thead_num = 20\n",
    "\thead_dim = 20\n",
    "\tattention_hidden_dim = 200\n",
    "\tdropout = 0.2\n",
    "\tbatch_size = 32\n",
    "\tverbose = False\n",
    "\n",
    "\tdef __str__(self):\n",
    "\t\treturn f\"\\n title_size: {self.title_size} \\n head_num: {self.head_num} \\n head_dim: {self.head_dim} \\n attention_hidden_dim: {self.attention_hidden_dim} \\n dropout: {self.dropout} \\n batch_size: {self.batch_size} \\n verbose: {self.verbose}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "\tdef __init__(self,hparams, verbose=False):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.head_num = hparams.head_num\n",
    "\t\tself.head_dim = hparams.head_dim\n",
    "\t\tself.output_dim = self.head_num * self.head_dim\n",
    "\t\tself.WQ = self.WK = self.WV = None\n",
    "\t\tself.dropout = nn.Dropout(hparams.dropout)\n",
    "\t\tself.verbose = verbose\n",
    "\n",
    "\tdef forward(self, Q_seq, K_seq, V_seq):\n",
    "\t\t# Lazy initialization of the weights\n",
    "\t\tif self.WQ is None:\n",
    "\t\t\tembedding_dim = Q_seq.size(-1)\n",
    "\t\t\tself.WQ = nn.Linear(embedding_dim, self.output_dim)\n",
    "\t\t\tself.WK = nn.Linear(embedding_dim, self.output_dim)\n",
    "\t\t\tself.WV = nn.Linear(embedding_dim, self.output_dim)\n",
    "\t\t\n",
    "\t\tQ = self.WQ(Q_seq)\n",
    "\t\tK = self.WK(K_seq)\n",
    "\t\tV = self.WV(V_seq)\n",
    "\n",
    "\t\tN, L, _ = Q.size()\n",
    "\t\tQ = Q.view(N, L, self.head_num, self.head_dim).transpose(1, 2)\n",
    "\t\tK = K.view(N, L, self.head_num, self.head_dim).transpose(1, 2)\n",
    "\t\tV = V.view(N, L, self.head_num, self.head_dim).transpose(1, 2)\n",
    "\n",
    "\t\tif self.verbose:\n",
    "\t\t\tprint(f\"Q shape: {Q.shape}\")\n",
    "\t\t\tprint(f\"K shape: {K.shape}\")\n",
    "\t\t\tprint(f\"V shape: {V.shape}\")\n",
    "\n",
    "\t\tscores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "\t\tattn = torch.softmax(scores, dim=-1)\n",
    "\t\tattn = self.dropout(attn)\n",
    "\n",
    "\t\toutput = torch.matmul(attn, V)\n",
    "\t\toutput = output.transpose(1, 2).contiguous().view(N, L, self.output_dim)\n",
    "\n",
    "\t\tif self.verbose:\n",
    "\t\t\tprint(f\"Attention shape: {attn.shape}\")\n",
    "\t\t\tprint(f\"Output shape: {output.shape}\")\n",
    "\n",
    "\t\treturn output\n",
    "\n",
    "class AttLayer(nn.Module):\n",
    "\tdef __init__(self, hparams, verbose=False):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.W = nn.Linear(hparams.head_num * hparams.head_dim, hparams.attention_hidden_dim)\n",
    "\t\tself.q = nn.Linear(hparams.attention_hidden_dim, 1, bias=False)\n",
    "\t\tself.dropout = nn.Dropout(hparams.dropout)\n",
    "\t\tself.verbose = verbose\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tattn = torch.tanh(self.W(x))\n",
    "\t\tattn = self.q(attn).squeeze(-1)\n",
    "\t\tattn = torch.softmax(attn, dim=1).unsqueeze(-1)\n",
    "\n",
    "\t\tif self.verbose:\n",
    "\t\t\tprint(f\"Attention weights shape: {attn.shape}\")\n",
    "\t\t\tprint(f\"Input shape: {x.shape}\")\n",
    "\n",
    "\t\toutput = torch.sum(x * attn, dim=1)\n",
    "\t\toutput = self.dropout(output)\n",
    "\n",
    "\t\tif self.verbose:\n",
    "\t\t\tprint(f\"Output shape: {output.shape}\")\n",
    "\n",
    "\t\treturn output\n",
    " \n",
    "class NRMSModel(nn.Module):\n",
    "\tdef __init__(self, hparams, word_embeddings):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.embedding = nn.Embedding.from_pretrained(\n",
    "\t\t\ttorch.FloatTensor(word_embeddings), freeze=False\n",
    "\t\t\t)\n",
    "\t\tself.dropout = nn.Dropout(hparams.dropout)\n",
    "\n",
    "\t\t# News Encoder\n",
    "\t\tself.news_self_att = SelfAttention(hparams, verbose=hparams.verbose)\n",
    "\t\tself.news_att = AttLayer(hparams, verbose=hparams.verbose)\n",
    "\n",
    "\t\t# User Encoder\n",
    "\t\tself.user_self_att = SelfAttention(hparams, verbose=hparams.verbose)\n",
    "\t\tself.user_att = AttLayer(hparams, verbose=hparams.verbose)\n",
    "\n",
    "\tdef encode_news(self, news_input):\n",
    "\t\tx = self.embedding(news_input)\n",
    "\t\tx = self.dropout(x)\n",
    "\t\tx = self.news_self_att(x, x, x)\n",
    "\t\tx = self.news_att(x)\n",
    "\t\treturn x\n",
    "\n",
    "\tdef encode_user(self, history_input):\n",
    "\t\tN, H, L = history_input.size()\n",
    "\t\thistory_input = history_input.view(N * H, L)\n",
    "\t\tnews_vectors = self.encode_news(history_input)\n",
    "\t\tnews_vectors = news_vectors.view(N, H, -1)\n",
    "\t\tuser_vector = self.user_self_att(news_vectors, news_vectors, news_vectors)\n",
    "\t\tuser_vector = self.user_att(user_vector)\n",
    "\t\treturn user_vector\n",
    "\n",
    "\tdef forward(self, his_input, pred_input):\n",
    "\t\tuser_vector = self.encode_user(his_input)\n",
    "\t\tN, M, L = pred_input.size()\n",
    "\t\tpred_input = pred_input.view(N * M, L)\n",
    "\t\tnews_vectors = self.encode_news(pred_input)\n",
    "\t\tnews_vectors = news_vectors.view(N, M, -1)\n",
    "\t\tscores = torch.bmm(news_vectors, user_vector.unsqueeze(2)).squeeze(-1)\n",
    "\t\treturn scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NRMSDataset(Dataset):\n",
    "\tdef __init__(self, df, article_mapping, title_size, history_column, candidate_column, verbose=False):\n",
    "\t\t\"\"\"\n",
    "\t\tArgs:\n",
    "\t\t\tdf (pl.DataFrame): DataFrame containing raw history and candidate article IDs.\n",
    "\t\t\tarticle_mapping (dict): Mapping of article IDs to tokenized representations.\n",
    "\t\t\ttitle_size (int): Maximum size of title tokens for padding/truncation.\n",
    "\t\t\thistory_column (str): Column containing user history.\n",
    "\t\t\tcandidate_column (str): Column containing candidate articles.\n",
    "\t\t\tverbose (bool): If True, prints debug information.\n",
    "\t\t\"\"\"\n",
    "\t\tself.history_raw = df[history_column].to_list()\n",
    "\t\tself.candidates_raw = df[candidate_column].to_list()\n",
    "\t\tself.labels = df[\"labels\"].to_list()\n",
    "\t\tself.article_mapping = article_mapping\n",
    "\t\tself.title_size = title_size\n",
    "\t\tself.verbose = verbose\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.labels)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t# Convert article IDs to tokenized representations\n",
    "\t\thistory_tokens = [\n",
    "\t\t\tself.article_mapping.get(aid, [0] * self.title_size) for aid in self.history_raw[idx]\n",
    "\t\t]\n",
    "\t\tcandidate_tokens = [\n",
    "\t\t\tself.article_mapping.get(aid, [0] * self.title_size) for aid in self.candidates_raw[idx]\n",
    "\t\t]\n",
    "\t\t\n",
    "\t\t# Convert to PyTorch tensors\n",
    "\t\this_ids = torch.tensor(history_tokens, dtype=torch.long)\n",
    "\t\tpred_ids = torch.tensor(candidate_tokens, dtype=torch.long)\n",
    "\t\ty = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "\t\tif self.verbose:\n",
    "\t\t\tprint(f\"History Tokens: {his_ids.shape}\")\n",
    "\t\t\tprint(f\"Candidate Tokens: {pred_ids.shape}\")\n",
    "\t\t\tprint(f\"Label: {y.shape}\")\n",
    "\n",
    "\t\treturn his_ids, pred_ids, y\n",
    "\n",
    "def nrms_collate_fn(batch):\n",
    "\thistories, candidates, labels = zip(*batch)\n",
    "\tmax_candidates = max([cand.size(0) for cand in candidates])\n",
    "\t\n",
    "\tpadded_candidates = []\n",
    "\tcandidate_masks = []\n",
    "\tfor cand in candidates:\n",
    "\t\tnum_cands = cand.size(0)\n",
    "\t\tif num_cands < max_candidates:\n",
    "\t\t\tpad_size = max_candidates - num_cands\n",
    "\t\t\tpadded_cand = torch.cat([cand, torch.zeros(pad_size, cand.size(1), dtype=torch.long)])\n",
    "\t\t\tmask = torch.cat([torch.ones(num_cands, dtype=torch.bool), torch.zeros(pad_size, dtype=torch.bool)])\n",
    "\t\telse:\n",
    "\t\t\tpadded_cand = cand[:max_candidates]\n",
    "\t\t\tmask = torch.ones(max_candidates, dtype=torch.bool)\n",
    "\t\tpadded_candidates.append(padded_cand)\n",
    "\t\tcandidate_masks.append(mask)\n",
    "\t\n",
    "\tpadded_candidates = torch.stack(padded_candidates)\n",
    "\tcandidate_masks = torch.stack(candidate_masks)\n",
    "\thistories = torch.stack(histories)\n",
    "\t\n",
    "\tpadded_labels = []\n",
    "\tfor label in labels:\n",
    "\t\tnum_cands = label.size(0)\n",
    "\t\tif num_cands < max_candidates:\n",
    "\t\t\tpad_size = max_candidates - num_cands\n",
    "\t\t\tpadded_label = torch.cat([label, torch.zeros(pad_size, dtype=torch.float32)])\n",
    "\t\telse:\n",
    "\t\t\tpadded_label = label[:max_candidates]\n",
    "\t\tpadded_labels.append(padded_label)\n",
    "\tpadded_labels = torch.stack(padded_labels)\n",
    "\t\n",
    "\treturn {\n",
    "\t\t'history': histories,\n",
    "\t\t'candidates': padded_candidates,\n",
    "\t\t'labels': padded_labels,\n",
    "\t\t'candidate_masks': candidate_masks\n",
    "\t}\n",
    "\n",
    "def create_dataloader(df, article_mapping, title_size, batch_size, history_column, candidate_column, shuffle=False):\n",
    "\tdataset = NRMSDataset(\n",
    "\t\tdf, article_mapping, title_size, history_column, candidate_column\n",
    "\t)\n",
    "\treturn DataLoader(\n",
    "\t\tdataset,\n",
    "\t\tbatch_size=batch_size,\n",
    "\t\tshuffle=shuffle,\n",
    "\t\tcollate_fn=nrms_collate_fn,\n",
    "\t\tnum_workers=0,  # Adjust based on your system\n",
    "\t)\n",
    "\n",
    "def prepare_df_for_training(df):\n",
    "\t\"\"\"\n",
    "\tValidate and preprocess DataFrame to ensure required columns exist.\n",
    "\t\"\"\"\n",
    "\trequired_columns = [DEFAULT_HISTORY_ARTICLE_ID_COL, DEFAULT_INVIEW_ARTICLES_COL, \"labels\"]\n",
    "\tfor col in required_columns:\n",
    "\t\tif col not in df.columns:\n",
    "\t\t\traise ValueError(f\"Missing required column: {col}\")\n",
    "\treturn df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Helper Function\n",
    "1. Optimizer: Adam\n",
    "2. Loss Function: Cross Entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(device, model, train_loader, val_loader, num_epochs, learning_rate=1e-3, patience=3):\n",
    "\tmodel = model.to(device)\n",
    "\n",
    "\tcriterion = nn.CrossEntropyLoss()\n",
    "\toptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\tbest_val_loss = float('inf')\n",
    "\tbest_val_auc = 0\n",
    "\tpatience_counter = 0\n",
    "\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\t# Training\n",
    "\t\tmodel.train()\n",
    "\t\ttotal_loss = 0\n",
    "\t\twith tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "\t\t\tfor batch in train_loader:\n",
    "\t\t\t\this_input = batch['history'].to(device)\n",
    "\t\t\t\tpred_input = batch['candidates'].to(device)\n",
    "\t\t\t\tlabels = batch['labels'].to(device)\n",
    "\t\t\t\tmasks = batch['candidate_masks'].to(device)\n",
    "\n",
    "\t\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\tscores = model(his_input, pred_input)\n",
    "\t\t\t\tscores = scores * masks\n",
    "\t\t\t\tloss = criterion(scores, labels)\n",
    "\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\ttotal_loss += loss.item()\n",
    "\t\t\t\tpbar.update(1)\n",
    "\n",
    "\t\tavg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "\t\t# Validation\n",
    "\t\tmodel.eval()\n",
    "\t\tval_loss = 0\n",
    "\t\tall_scores = []\n",
    "\t\tall_labels = []\n",
    "\t\twith torch.no_grad(), tqdm(total=len(val_loader), desc=\"Validation\", unit=\"batch\") as pbar:\n",
    "\t\t\tfor batch in val_loader:\n",
    "\t\t\t\this_input = batch['history'].to(device)\n",
    "\t\t\t\tpred_input = batch['candidates'].to(device)\n",
    "\t\t\t\tlabels = batch['labels']\n",
    "\t\t\t\tmasks = batch['candidate_masks']\n",
    "\n",
    "\t\t\t\tscores = model(his_input, pred_input)\n",
    "\t\t\t\tscores = scores * masks\n",
    "\t\t\t\tloss = criterion(scores, labels.to(device))\n",
    "\t\t\t\tval_loss += loss.item()\n",
    "\n",
    "\t\t\t\tvalid_scores = scores[masks.bool()].cpu().numpy()\n",
    "\t\t\t\tvalid_labels = labels[masks.bool()].numpy()\n",
    "\t\t\t\tall_scores.extend(valid_scores)\n",
    "\t\t\t\tall_labels.extend(valid_labels)\n",
    "\t\t\t\tpbar.update(1)\n",
    "\n",
    "\t\tavg_val_loss = val_loss / len(val_loader)\n",
    "\t\tval_auc = roc_auc_score(all_labels, all_scores)\n",
    "\t\tauc_improvement = val_auc - best_val_auc if epoch > 0 else val_auc\n",
    "\t\tprint(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val AUC: {val_auc:.4f}, Improvement from Previous Epoch: {auc_improvement:.4f}\")\n",
    "\t\tbest_val_auc = max(best_val_auc, val_auc)\n",
    "\n",
    "\n",
    "\t\t# Early stopping\n",
    "\t\tif avg_val_loss < best_val_loss:\n",
    "\t\t\tbest_val_loss = avg_val_loss\n",
    "\t\t\tpatience_counter = 0\n",
    "\t\telse:\n",
    "\t\t\tpatience_counter += 1\n",
    "\t\t\tif patience_counter >= patience:\n",
    "\t\t\t\tprint(\"Early stopping triggered\")\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: \n",
      " title_size: 30 \n",
      " head_num: 20 \n",
      " head_dim: 20 \n",
      " attention_hidden_dim: 200 \n",
      " dropout: 0.2 \n",
      " batch_size: 32 \n",
      " verbose: False\n"
     ]
    }
   ],
   "source": [
    "# Setting hyperparameters\n",
    "hparams=HParams()\n",
    "print(\"Hyperparameters:\", hparams)\n",
    "\n",
    "# For loading data\n",
    "DATA_FRACTION = 0.05 # Fraction of data to use for training\n",
    "SAMPLING_NPRATIO = 4 # For every positive sample ( a click ), we sample X negative samples\n",
    "HISTORY_SIZE = 20 # History of each users interactions will be limited to the most recent X articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer\n",
    "TRANSFORMER_MODEL_NAME = \"facebookai/xlm-roberta-base\"\n",
    "\n",
    "# Data loading\n",
    "PATH = Path(\"~/Git Repositories/ebnerd-benchmark/data\").expanduser()\n",
    "DATASPLIT = \"ebnerd_small\"\n",
    "\n",
    "# Load and process training data\n",
    "df_train = (\n",
    "\tebnerd_from_path(\n",
    "\t\tPATH.joinpath(DATASPLIT, \"train\"),\n",
    "\t\thistory_size=HISTORY_SIZE,\n",
    "\t\tpadding=0,\n",
    "\t)\n",
    "\t.pipe(\n",
    "\t\tsampling_strategy_wu2019,\n",
    "\t\tnpratio=SAMPLING_NPRATIO,\n",
    "\t\twith_replacement=True,\n",
    "\t\tseed=seed,\n",
    "\t)\n",
    "\t.pipe(create_binary_labels_column)\n",
    "\t.sample(fraction=DATA_FRACTION)\n",
    ")\n",
    "\n",
    "# Split into train/validation\n",
    "dt_split = df_train[DEFAULT_IMPRESSION_TIMESTAMP_COL].max() - datetime.timedelta(days=1)\n",
    "df_train_split = df_train.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL) < dt_split)\n",
    "df_validation = df_train.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL) >= dt_split)\n",
    "\n",
    "# Load articles and prepare embeddings\n",
    "df_articles = pl.read_parquet(PATH.joinpath(\"articles.parquet\"))\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "word_embeddings = transformer_model.get_input_embeddings().weight.detach().numpy()\n",
    "\n",
    "# Prepare article embeddings\n",
    "df_articles, cat_col = concat_str_columns(df_articles, columns=[\"subtitle\", \"title\"])\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "\tdf_articles, \n",
    "\ttransformer_tokenizer, \n",
    "\tcat_col, \n",
    "\tmax_length=hparams.title_size\n",
    ")\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "\tdf=df_articles, \n",
    "\tvalue_col=token_col_title\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:  37%|███▋      | 117/317 [01:40<02:46,  1.20batch/s]"
     ]
    }
   ],
   "source": [
    "# Validate DataFrames\n",
    "df_train_split = prepare_df_for_training(df_train_split)\n",
    "df_validation = prepare_df_for_training(df_validation)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = create_dataloader(\n",
    "\tdf_train_split, article_mapping, hparams.title_size,\n",
    "\tbatch_size=hparams.batch_size,\n",
    "\thistory_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "\tcandidate_column=DEFAULT_INVIEW_ARTICLES_COL,\n",
    "\tshuffle=True\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader(\n",
    "\tdf_validation, article_mapping, hparams.title_size,\n",
    "\tbatch_size=hparams.batch_size,\n",
    "\thistory_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "\tcandidate_column=DEFAULT_INVIEW_ARTICLES_COL,\n",
    "\tshuffle=False\n",
    ")\n",
    "\n",
    "# Initialize and train model\n",
    "device = torch.device('cunda' if torch.cuda.is_available() else 'cpu')\n",
    "model = NRMSModel(hparams, word_embeddings)\n",
    "\n",
    "model = train_and_evaluate(\n",
    "  device,\n",
    "\tmodel, \n",
    "\ttrain_loader, \n",
    "\tval_loader, \n",
    "\tnum_epochs=2, \n",
    "\tlearning_rate=1e-3, \n",
    "\tpatience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "auc: 0.5480\n",
      "mrr: 0.0046\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "\tmodel.eval()\n",
    "\tall_scores = []\n",
    "\tall_labels = []\n",
    "\twith torch.no_grad():\n",
    "\t\tfor batch in dataloader:\n",
    "\t\t\this_input = batch['history'].to(device)\n",
    "\t\t\tpred_input = batch['candidates'].to(device)\n",
    "\t\t\tlabels = batch['labels']\n",
    "\t\t\tmasks = batch['candidate_masks']\n",
    "\n",
    "\t\t\tscores = model(his_input, pred_input)\n",
    "\t\t\tvalid_scores = scores[masks.bool()].cpu().numpy()\n",
    "\t\t\tvalid_labels = labels[masks.bool()].numpy()\n",
    "\t\t\tall_scores.extend(valid_scores)\n",
    "\t\t\tall_labels.extend(valid_labels)\n",
    "\n",
    "\tall_scores = np.array(all_scores)\n",
    "\tall_labels = np.array(all_labels)\n",
    "\n",
    "\tmetrics = {\n",
    "\t\t'auc': roc_auc_score(all_labels, all_scores),\n",
    "\t\t'mrr': mrr_score(all_labels, all_scores)\n",
    "\t}\n",
    "\treturn metrics\n",
    "\n",
    "# Evaluate model\n",
    "metrics = evaluate_model(model, val_loader, device)\n",
    "print(\"\\nValidation Metrics:\")\n",
    "for metric_name, value in metrics.items():\n",
    "\tprint(f\"{metric_name}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eb-nerd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
