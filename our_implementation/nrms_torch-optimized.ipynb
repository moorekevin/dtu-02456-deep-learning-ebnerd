{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NRMS Model (PyTorch Version)\n",
    "\n",
    "This notebook demonstrates how to build, train, and evaluate a Neural News Recommendation Model (NRMS) using PyTorch instead of TensorFlow. We will still attempt to use `ebrec` utilities for data loading and evaluation where possible.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We will:\n",
    "1.  Setup: Import necessary libraries and define hyperparameters.\n",
    "2.  Define NRMS Model Components: Implement custom layers and the NRMS model architecture.\n",
    "3.  Data Loading and Preparation: Load and preprocess the dataset.\n",
    "4.  Article Embeddings: Generate embeddings for articles using a pre-trained transformer model.\n",
    "5.  Batch and Shape Data: Create PyTorch datasets and dataloaders.\n",
    "6.  Training the Model: Train the NRMS model.\n",
    "7.  Evaluation on Test Set: Evaluate the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinmoore/anaconda3/envs/eb-nerd/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from ebrec.utils._behaviors import ebnerd_from_path, create_binary_labels_column, sampling_strategy_wu2019\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers, create_article_id_to_value_mapping\n",
    "from ebrec.utils._polars import concat_str_columns\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_USER_COL, DEFAULT_IMPRESSION_ID_COL, DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL, DEFAULT_CLICKED_ARTICLES_COL, DEFAULT_INVIEW_ARTICLES_COL\n",
    ")\n",
    "from ebrec.evaluation.metrics._ranking import ndcg_score, mrr_score\n",
    "\n",
    "# Set random seed\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams:\n",
    "\ttitle_size = 30\n",
    "\thead_num = 12\n",
    "\thead_dim = 64\n",
    "\tattention_hidden_dim = 200\n",
    "\tdropout = 0.2\n",
    "\tbatch_size = 32\n",
    "\n",
    "hparams = HParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    " def __init__(self, head_num, head_dim, embedding_dim):\n",
    "   super().__init__()\n",
    "   self.head_num = head_num\n",
    "   self.head_dim = head_dim\n",
    "   self.output_dim = head_num * head_dim\n",
    "   self.WQ = nn.Linear(embedding_dim, self.output_dim)\n",
    "   self.WK = nn.Linear(embedding_dim, self.output_dim)\n",
    "   self.WV = nn.Linear(embedding_dim, self.output_dim)\n",
    "   self.dropout = nn.Dropout(hparams.dropout)\n",
    "\n",
    " def forward(self, Q_seq, K_seq, V_seq):\n",
    "   Q = self.WQ(Q_seq)\n",
    "   K = self.WK(K_seq)\n",
    "   V = self.WV(V_seq)\n",
    "   \n",
    "   N, L, _ = Q.size()\n",
    "   Q = Q.view(N, L, self.head_num, self.head_dim).transpose(1, 2)\n",
    "   K = K.view(N, L, self.head_num, self.head_dim).transpose(1, 2)\n",
    "   V = V.view(N, L, self.head_num, self.head_dim).transpose(1, 2)\n",
    "   \n",
    "   scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)\n",
    "   attn = torch.softmax(scores, dim=-1)\n",
    "   attn = self.dropout(attn)\n",
    "   \n",
    "   output = torch.matmul(attn, V)\n",
    "   output = output.transpose(1, 2).contiguous().view(N, L, self.output_dim)\n",
    "   return output\n",
    "\n",
    "class AttLayer(nn.Module):\n",
    " def __init__(self, attention_hidden_dim):\n",
    "   super().__init__()\n",
    "   self.W = nn.Linear(hparams.head_num * hparams.head_dim, attention_hidden_dim)\n",
    "   self.q = nn.Linear(attention_hidden_dim, 1, bias=False)\n",
    "   self.dropout = nn.Dropout(hparams.dropout)\n",
    "\n",
    " def forward(self, x):\n",
    "   attn = torch.tanh(self.W(x))\n",
    "   attn = self.q(attn).squeeze(-1)\n",
    "   attn = torch.softmax(attn, dim=1).unsqueeze(-1)\n",
    "   output = torch.sum(x * attn, dim=1)\n",
    "   output = self.dropout(output)\n",
    "   return output\n",
    " \n",
    "class NRMSModel(nn.Module):\n",
    " def __init__(self, hparams, word_embeddings):\n",
    "   super().__init__()\n",
    "   self.embedding = nn.Embedding.from_pretrained(\n",
    "     torch.FloatTensor(word_embeddings), freeze=False\n",
    "   )\n",
    "   self.dropout = nn.Dropout(hparams.dropout)\n",
    "\n",
    "   # News Encoder\n",
    "   self.news_self_att = SelfAttention(hparams.head_num, hparams.head_dim, embedding_dim=768)\n",
    "   self.news_att = AttLayer(hparams.attention_hidden_dim)\n",
    "\n",
    "   # User Encoder\n",
    "   self.user_self_att = SelfAttention(hparams.head_num, hparams.head_dim, embedding_dim=768)\n",
    "   self.user_att = AttLayer(hparams.attention_hidden_dim)\n",
    "\n",
    " def encode_news(self, news_input):\n",
    "   x = self.embedding(news_input)\n",
    "   x = self.dropout(x)\n",
    "   x = self.news_self_att(x, x, x)\n",
    "   x = self.news_att(x)\n",
    "   return x\n",
    "\n",
    " def encode_user(self, history_input):\n",
    "   N, H, L = history_input.size()\n",
    "   history_input = history_input.view(N * H, L)\n",
    "   news_vectors = self.encode_news(history_input)\n",
    "   news_vectors = news_vectors.view(N, H, -1)\n",
    "   user_vector = self.user_self_att(news_vectors, news_vectors, news_vectors)\n",
    "   user_vector = self.user_att(user_vector)\n",
    "   return user_vector\n",
    "\n",
    " def forward(self, his_input, pred_input):\n",
    "   user_vector = self.encode_user(his_input)\n",
    "   N, M, L = pred_input.size()\n",
    "   pred_input = pred_input.view(N * M, L)\n",
    "   news_vectors = self.encode_news(pred_input)\n",
    "   news_vectors = news_vectors.view(N, M, -1)\n",
    "   scores = torch.bmm(news_vectors, user_vector.unsqueeze(2)).squeeze(-1)\n",
    "   return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NRMSDataset(Dataset):\n",
    "  def __init__(self, df, article_mapping, title_size):\n",
    "    self.history = df[\"history_tokens\"].to_list()\n",
    "    self.candidates = df[\"candidate_tokens\"].to_list()\n",
    "    self.labels = df[\"labels\"].to_list()\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    his_ids = torch.tensor(self.history[idx], dtype=torch.long)\n",
    "    pred_ids = torch.tensor(self.candidates[idx], dtype=torch.long)\n",
    "    y = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "    return his_ids, pred_ids, y\n",
    "\n",
    "def nrms_collate_fn(batch):\n",
    "  histories, candidates, labels = zip(*batch)\n",
    "  max_candidates = max([cand.size(0) for cand in candidates])\n",
    "  \n",
    "  padded_candidates = []\n",
    "  candidate_masks = []\n",
    "  for cand in candidates:\n",
    "    num_cands = cand.size(0)\n",
    "    if num_cands < max_candidates:\n",
    "      pad_size = max_candidates - num_cands\n",
    "      padded_cand = torch.cat([cand, torch.zeros(pad_size, cand.size(1), dtype=torch.long)])\n",
    "      mask = torch.cat([torch.ones(num_cands, dtype=torch.bool), torch.zeros(pad_size, dtype=torch.bool)])\n",
    "    else:\n",
    "      padded_cand = cand[:max_candidates]\n",
    "      mask = torch.ones(max_candidates, dtype=torch.bool)\n",
    "    padded_candidates.append(padded_cand)\n",
    "    candidate_masks.append(mask)\n",
    "  \n",
    "  padded_candidates = torch.stack(padded_candidates)\n",
    "  candidate_masks = torch.stack(candidate_masks)\n",
    "  histories = torch.stack(histories)\n",
    "  \n",
    "  padded_labels = []\n",
    "  for label in labels:\n",
    "    num_cands = label.size(0)\n",
    "    if num_cands < max_candidates:\n",
    "      pad_size = max_candidates - num_cands\n",
    "      padded_label = torch.cat([label, torch.zeros(pad_size, dtype=torch.float32)])\n",
    "    else:\n",
    "      padded_label = label[:max_candidates]\n",
    "    padded_labels.append(padded_label)\n",
    "  padded_labels = torch.stack(padded_labels)\n",
    "  \n",
    "  return {\n",
    "    'history': histories,\n",
    "    'candidates': padded_candidates,\n",
    "    'labels': padded_labels,\n",
    "    'candidate_masks': candidate_masks\n",
    "  }\n",
    "\n",
    "def create_dataloader(df, article_mapping, title_size, batch_size=32, shuffle=False):\n",
    "  dataset = NRMSDataset(df, article_mapping, title_size)\n",
    "  return DataLoader(\n",
    "    dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle,\n",
    "    collate_fn=nrms_collate_fn,\n",
    "    num_workers=0  # Adjust based on your system\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer\n",
    "TRANSFORMER_MODEL_NAME = \"facebookai/xlm-roberta-base\"\n",
    "\n",
    "# Data loading\n",
    "PATH = Path(\"~/Git Repositories/ebnerd-benchmark/data\").expanduser()\n",
    "DATASPLIT = \"ebnerd_small\"\n",
    "\n",
    "DATA_FRACTION = 0.01\n",
    "SAMPLING_NPRATIO = 4\n",
    "HISTORY_SIZE = 20\n",
    "# Load and process training data\n",
    "df_train = (\n",
    "  ebnerd_from_path(\n",
    "    PATH.joinpath(DATASPLIT, \"train\"),\n",
    "    history_size=HISTORY_SIZE,\n",
    "    padding=0,\n",
    "  )\n",
    "  .pipe(\n",
    "    sampling_strategy_wu2019,\n",
    "    npratio=SAMPLING_NPRATIO,\n",
    "    with_replacement=True,\n",
    "    seed=seed,\n",
    "  )\n",
    "  .pipe(create_binary_labels_column)\n",
    "  .sample(fraction=DATA_FRACTION)\n",
    ")\n",
    "\n",
    "# Split into train/validation\n",
    "dt_split = df_train[DEFAULT_IMPRESSION_TIMESTAMP_COL].max() - datetime.timedelta(days=1)\n",
    "df_train_split = df_train.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL) < dt_split)\n",
    "df_validation = df_train.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL) >= dt_split)\n",
    "\n",
    "# Load articles and prepare embeddings\n",
    "df_articles = pl.read_parquet(PATH.joinpath(\"articles.parquet\"))\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "word_embeddings = transformer_model.get_input_embeddings().weight.detach().numpy()\n",
    "\n",
    "# Prepare article embeddings\n",
    "df_articles, cat_col = concat_str_columns(df_articles, columns=[\"subtitle\", \"title\"])\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "  df_articles, \n",
    "  transformer_tokenizer, \n",
    "  cat_col, \n",
    "  max_length=hparams.title_size\n",
    ")\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "  df=df_articles, \n",
    "  value_col=token_col_title\n",
    ")\n",
    "\n",
    "# Add preprocessing function\n",
    "def prepare_df_for_training(df, article_mapping, history_column=DEFAULT_HISTORY_ARTICLE_ID_COL):\n",
    "  \"\"\"Convert article IDs to tokens using the mapping\"\"\"\n",
    "  history_tokens = [\n",
    "    [article_mapping.get(aid, [0] * hparams.title_size) for aid in hist]\n",
    "    for hist in df[history_column].to_list()\n",
    "  ]\n",
    "  \n",
    "  candidate_tokens = [\n",
    "    [article_mapping.get(aid, [0] * hparams.title_size) for aid in cands]\n",
    "    for cands in df[DEFAULT_INVIEW_ARTICLES_COL].to_list()\n",
    "  ]\n",
    "  \n",
    "  return (\n",
    "    df.with_columns([\n",
    "      pl.Series(\"history_tokens\", history_tokens),\n",
    "      pl.Series(\"candidate_tokens\", candidate_tokens)\n",
    "    ])\n",
    "  )\n",
    "\n",
    "# Apply preprocessing\n",
    "df_train_split = prepare_df_for_training(df_train_split, article_mapping)\n",
    "df_validation = prepare_df_for_training(df_validation, article_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function\n",
    "1. Optimizer: Adam\n",
    "2. Loss Function: Cross Entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, train_loader, val_loader, num_epochs, learning_rate=1e-3, patience=3):\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  model = model.to(device)\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  best_val_loss = float('inf')\n",
    "  best_val_auc = 0\n",
    "  patience_counter = 0\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "      his_input = batch['history'].to(device)\n",
    "      pred_input = batch['candidates'].to(device)\n",
    "      labels = batch['labels'].to(device)\n",
    "      masks = batch['candidate_masks'].to(device)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      scores = model(his_input, pred_input)\n",
    "      scores = scores * masks\n",
    "      loss = criterion(scores, labels)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "      for batch in val_loader:\n",
    "        his_input = batch['history'].to(device)\n",
    "        pred_input = batch['candidates'].to(device)\n",
    "        labels = batch['labels']\n",
    "        masks = batch['candidate_masks']\n",
    "\n",
    "        scores = model(his_input, pred_input)\n",
    "        scores = scores * masks\n",
    "        loss = criterion(scores, labels.to(device))\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        valid_scores = scores[masks.bool()].cpu().numpy()\n",
    "        valid_labels = labels[masks.bool()].numpy()\n",
    "        all_scores.extend(valid_scores)\n",
    "        all_labels.extend(valid_labels)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_auc = roc_auc_score(all_labels, all_scores)\n",
    "    auc_improvement = val_auc - best_val_auc if epoch > 0 else val_auc\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val AUC: {val_auc:.4f}, Improvement from Previous Epoch: {auc_improvement:.4f}\")\n",
    "    best_val_auc = max(best_val_auc, val_auc)\n",
    "\n",
    "\n",
    "    # Early stopping\n",
    "    if avg_val_loss < best_val_loss:\n",
    "      best_val_loss = avg_val_loss\n",
    "      patience_counter = 0\n",
    "    else:\n",
    "      patience_counter += 1\n",
    "      if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Train Loss: 1.4497, Val Loss: 1.7211, Val AUC: 0.5423, Improvement from Previous Epoch: 0.5423\n",
      "Epoch 2/2, Train Loss: 1.3180, Val Loss: 1.6864, Val AUC: 0.5480, Improvement from Previous Epoch: 0.0057\n"
     ]
    }
   ],
   "source": [
    "# Create dataloaders\n",
    "train_loader = create_dataloader(\n",
    "  df_train_split,\n",
    "  article_mapping,\n",
    "  hparams.title_size,\n",
    "  batch_size=hparams.batch_size,\n",
    "  shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader(\n",
    "  df_validation,\n",
    "  article_mapping,\n",
    "  hparams.title_size,\n",
    "  batch_size=hparams.batch_size,\n",
    "  shuffle=False\n",
    ")\n",
    "\n",
    "# Initialize and train model\n",
    "model = NRMSModel(hparams, word_embeddings)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = train_and_evaluate(\n",
    "  model, \n",
    "  train_loader, \n",
    "  val_loader, \n",
    "  num_epochs=2, \n",
    "  learning_rate=1e-3, \n",
    "  patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "auc: 0.5480\n",
      "mrr: 0.0046\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "  model.eval()\n",
    "  all_scores = []\n",
    "  all_labels = []\n",
    "  with torch.no_grad():\n",
    "    for batch in dataloader:\n",
    "      his_input = batch['history'].to(device)\n",
    "      pred_input = batch['candidates'].to(device)\n",
    "      labels = batch['labels']\n",
    "      masks = batch['candidate_masks']\n",
    "\n",
    "      scores = model(his_input, pred_input)\n",
    "      valid_scores = scores[masks.bool()].cpu().numpy()\n",
    "      valid_labels = labels[masks.bool()].numpy()\n",
    "      all_scores.extend(valid_scores)\n",
    "      all_labels.extend(valid_labels)\n",
    "\n",
    "  all_scores = np.array(all_scores)\n",
    "  all_labels = np.array(all_labels)\n",
    "\n",
    "  metrics = {\n",
    "    'auc': roc_auc_score(all_labels, all_scores),\n",
    "    'mrr': mrr_score(all_labels, all_scores)\n",
    "  }\n",
    "  return metrics\n",
    "\n",
    "# Evaluate model\n",
    "metrics = evaluate_model(model, val_loader, device)\n",
    "print(\"\\nValidation Metrics:\")\n",
    "for metric_name, value in metrics.items():\n",
    "  print(f\"{metric_name}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eb-nerd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
