{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NRMS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import tensorflow as tf\n",
    "##############################################################\n",
    "# Simple NRMS-like Model\n",
    "##############################################################\n",
    "keras = tf.keras\n",
    "backend = keras.backend\n",
    "layers = keras.layers\n",
    "class AttLayer2(layers.Layer):\n",
    "\tdef __init__(self, dim=200, seed=0, **kwargs):\n",
    "\t\tself.dim = dim\n",
    "\t\tself.seed = seed\n",
    "\t\tsuper(AttLayer2, self).__init__(**kwargs)\n",
    "\t\n",
    "\tdef build(self, input_shape):\n",
    "\t\tself.W = self.add_weight(\n",
    "\t\t\tname=\"W\",\n",
    "\t\t\tshape=(int(input_shape[-1]), self.dim),\n",
    "\t\t\tinitializer=\"glorot_uniform\",\n",
    "\t\t\ttrainable=True,\n",
    "\t\t)\n",
    "\t\tself.b = self.add_weight(\n",
    "\t\t\tname=\"b\",\n",
    "\t\t\tshape=(self.dim,),\n",
    "\t\t\tinitializer=\"zeros\",\n",
    "\t\t\ttrainable=True,\n",
    "\t\t)\n",
    "\t\tself.q = self.add_weight(\n",
    "\t\t\tname=\"q\",\n",
    "\t\t\tshape=(self.dim, 1),\n",
    "\t\t\tinitializer=\"glorot_uniform\",\n",
    "\t\t\ttrainable=True,\n",
    "\t\t)\n",
    "\t\tsuper(AttLayer2, self).build(input_shape)\n",
    "\n",
    "\tdef call(self, inputs, mask=None):\n",
    "\t\tattention = backend.tanh(tf.tensordot(inputs, self.W, axes=[[2],[0]]) + self.b)\n",
    "\t\tattention = backend.dot(attention, self.q)\n",
    "\t\tattention = backend.squeeze(attention, axis=2)\n",
    "\t\tif mask == None:\n",
    "\t\t\tattention = backend.exp(attention)\n",
    "\t\telse:\n",
    "\t\t\tattention = backend.exp(attention) * backend.mask(mask, dtype=\"float32\")\n",
    "\t\tattention_weight = attention / (backend.sum(attention, axis=-1, keepdims=True) + backend.epsilon())\n",
    "\t\tattention_weight = backend.expand_dims(attention_weight)\n",
    "\t\tweighted_input = inputs * attention_weight\n",
    "\t\treturn backend.sum(weighted_input, axis=1)\n",
    "\n",
    "class SelfAttention(layers.Layer):\n",
    "\tdef __init__(self, head_num, head_dim, seed=0, mask_right=False, **kwargs):\n",
    "\t\tself.head_num = head_num\n",
    "\t\tself.head_dim = head_dim\n",
    "\t\tself.output_dim = head_num * head_dim\n",
    "\t\tself.mask_right = mask_right\n",
    "\t\tself.seed = seed\n",
    "\t\tsuper(SelfAttention, self).__init__(**kwargs)\n",
    "\n",
    "\tdef build(self, input_shape):\n",
    "\t\tglorot_uniform = keras.initializers.glorot_uniform(seed=self.seed)\n",
    "\t\tself.WQ = self.add_weight(\n",
    "\t\t\tname=\"WQ\",\n",
    "\t\t\tshape=(int(input_shape[0][-1]), self.output_dim),\n",
    "\t\t\tinitializer=glorot_uniform,\n",
    "\t\t\ttrainable=True,\n",
    "\t\t)\n",
    "\t\tself.WK = self.add_weight(\n",
    "\t\t\tname=\"WK\",\n",
    "\t\t\tshape=(int(input_shape[1][-1]), self.output_dim),\n",
    "\t\t\tinitializer=glorot_uniform,\n",
    "\t\t\ttrainable=True,\n",
    "\t\t)\n",
    "\t\tself.WV = self.add_weight(\n",
    "\t\t\tname=\"WV\",\n",
    "\t\t\tshape=(int(input_shape[2][-1]), self.output_dim),\n",
    "\t\t\tinitializer=glorot_uniform,\n",
    "\t\t\ttrainable=True,\n",
    "\t\t)\n",
    "\t\tsuper(SelfAttention, self).build(input_shape)\n",
    "\n",
    "\tdef Mask(self, inputs, seq_len, mode=\"add\"):\n",
    "\t\tif seq_len is None:\n",
    "\t\t\treturn inputs\n",
    "\t\telse:\n",
    "\t\t\tmask = backend.one_hot(indices=seq_len[:, 0], num_classes=backend.shape(inputs)[1])\n",
    "\t\t\tmask = 1 - backend.cumsum(mask, axis=1)\n",
    "\n",
    "\t\t\tfor _ in range(len(inputs.shape) - 2):\n",
    "\t\t\t\tmask = backend.expand_dims(mask, 2)\n",
    "\n",
    "\t\t\tif mode == \"mul\":\n",
    "\t\t\t\treturn inputs * mask\n",
    "\t\t\telif mode == \"add\":\n",
    "\t\t\t\treturn inputs - (1 - mask) * 1e12\n",
    "\n",
    "\tdef call(self, inputs):\n",
    "\t\tif len(inputs) == 3:\n",
    "\t\t\tQ_seq, K_seq, V_seq = inputs\n",
    "\t\t\tQ_len, V_len = None, None\n",
    "\t\telif len(inputs) == 5:\n",
    "\t\t\tQ_seq, K_seq, V_seq, Q_len, V_len = inputs\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(\"SelfAttention layer expected inputs of length 3 or 5.\")\n",
    "\t\t\n",
    "\t\tQ_seq = backend.dot(Q_seq, self.WQ)\n",
    "\t\tQ_seq = backend.reshape(\n",
    "\t\t\tQ_seq, (-1, backend.shape(Q_seq)[1], self.head_num, self.head_dim)\n",
    "\t\t)\n",
    "\t\tQ_seq = backend.permute_dimensions(Q_seq, pattern=(0, 2, 1, 3))\n",
    "\n",
    "\t\tK_seq = backend.dot(K_seq, self.WK)\n",
    "\t\tK_seq = backend.reshape(\n",
    "\t\t\t\tK_seq, shape=(-1, backend.shape(K_seq)[1], self.head_num, self.head_dim)\n",
    "\t\t)\n",
    "\t\tK_seq = backend.permute_dimensions(K_seq, pattern=(0, 2, 1, 3))\n",
    "\n",
    "\t\tV_seq = backend.dot(V_seq, self.WV)\n",
    "\t\tV_seq = backend.reshape(\n",
    "\t\t\t\tV_seq, shape=(-1, backend.shape(V_seq)[1], self.head_num, self.head_dim)\n",
    "\t\t)\n",
    "\t\tV_seq = backend.permute_dimensions(V_seq, pattern=(0, 2, 1, 3))\n",
    "\t\tA = tf.matmul(Q_seq, K_seq, adjoint_a=False, adjoint_b=True) / backend.sqrt(\n",
    "\t\t\t\tbackend.cast(self.head_dim, dtype=\"float32\")\n",
    "\t\t)\n",
    "\n",
    "\t\tA = backend.permute_dimensions(\n",
    "\t\t\t\tA, pattern=(0, 3, 2, 1)\n",
    "\t\t) \n",
    "\n",
    "\t\tA = self.Mask(A, V_len, \"add\")\n",
    "\t\tA = backend.permute_dimensions(A, pattern=(0, 3, 2, 1))\n",
    "\n",
    "\t\tif self.mask_right:\n",
    "\t\t\t\tones = backend.ones_like(A[:1, :1])\n",
    "\t\t\t\tlower_triangular = backend.tf.matrix_band_part(ones, num_lower=-1, num_upper=0)\n",
    "\t\t\t\tmask = (ones - lower_triangular) * 1e12\n",
    "\t\t\t\tA = A - mask\n",
    "\t\tA = backend.softmax(A)\n",
    "\n",
    "\t\tO_seq = tf.matmul(A, V_seq, adjoint_a=True, adjoint_b=False)\n",
    "\t\tO_seq = backend.permute_dimensions(O_seq, pattern=(0, 2, 1, 3))\n",
    "\n",
    "\t\tO_seq = backend.reshape(O_seq, shape=(-1, backend.shape(O_seq)[1], self.output_dim))\n",
    "\t\tO_seq = self.Mask(O_seq, Q_len, \"mul\")\n",
    "\t\treturn O_seq\n",
    "\n",
    "class NRMSModel:\n",
    "\t\"\"\"NRMS model(Neural News Recommendation with Multi-Head Self-Attention)\n",
    "\n",
    "\tChuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang,and Xing Xie, \"Neural News\n",
    "\tRecommendation with Multi-Head Self-Attention\" in Proceedings of the 2019 Conference\n",
    "\ton Empirical Methods in Natural Language Processing and the 9th International Joint Conference\n",
    "\ton Natural Language Processing (EMNLP-IJCNLP)\n",
    "\n",
    "\tAttributes:\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(\n",
    "\t\t\tself,\n",
    "\t\t\thparams: dict,\n",
    "\t\t\tword2vec_embedding: np.ndarray = None,\n",
    "\t\t\tword_emb_dim: int = 300,\n",
    "\t\t\tvocab_size: int = 32000,\n",
    "\t\t\tseed: int = None,\n",
    "\t):\n",
    "\t\t\t\"\"\"Initialization steps for NRMS.\"\"\"\n",
    "\t\t\tself.hparams = hparams\n",
    "\t\t\tself.seed = seed\n",
    "\n",
    "\t\t\t# SET SEED:\n",
    "\t\t\ttf.random.set_seed(seed)\n",
    "\t\t\tnp.random.seed(seed)\n",
    "\n",
    "\t\t\t# INIT THE WORD-EMBEDDINGS:\n",
    "\t\t\tif word2vec_embedding is None:\n",
    "\t\t\t\t\t# Xavier Initialization\n",
    "\t\t\t\t\tinitializer = GlorotUniform(seed=self.seed)\n",
    "\t\t\t\t\tself.word2vec_embedding = initializer(shape=(vocab_size, word_emb_dim))\n",
    "\t\t\t\t\t# self.word2vec_embedding = np.random.rand(vocab_size, word_emb_dim)\n",
    "\t\t\telse:\n",
    "\t\t\t\t\tself.word2vec_embedding = word2vec_embedding\n",
    "\n",
    "\t\t\t# BUILD AND COMPILE MODEL:\n",
    "\t\t\tself.model, self.scorer = self._build_graph()\n",
    "\t\t\tdata_loss = self._get_loss(self.hparams.loss)\n",
    "\t\t\ttrain_optimizer = self._get_opt(\n",
    "\t\t\t\t\toptimizer=self.hparams.optimizer, lr=self.hparams.learning_rate\n",
    "\t\t\t)\n",
    "\t\t\tself.model.compile(loss=data_loss, optimizer=train_optimizer)\n",
    "\n",
    "\tdef _get_loss(self, loss: str):\n",
    "\t\t\t\"\"\"Make loss function, consists of data loss and regularization loss\n",
    "\t\t\tReturns:\n",
    "\t\t\t\t\tobject: Loss function or loss function name\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tif loss == \"cross_entropy_loss\":\n",
    "\t\t\t\t\tdata_loss = \"categorical_crossentropy\"\n",
    "\t\t\telif loss == \"log_loss\":\n",
    "\t\t\t\t\tdata_loss = \"binary_crossentropy\"\n",
    "\t\t\telse:\n",
    "\t\t\t\t\traise ValueError(f\"this loss not defined {loss}\")\n",
    "\t\t\treturn data_loss\n",
    "\n",
    "\tdef _get_opt(self, optimizer: str, lr: float):\n",
    "\t\t\"\"\"Get the optimizer according to configuration. Usually we will use Adam.\n",
    "\t\tReturns:\n",
    "\t\t\t\tobject: An optimizer.\n",
    "\t\t\"\"\"\n",
    "\t\tif optimizer == \"adam\":\n",
    "\t\t\ttrain_opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\t\telse:\n",
    "\t\t\traise ValueError(f\"this optimizer not defined {optimizer}\")\n",
    "\t\treturn train_opt\n",
    "\n",
    "\tdef _build_graph(self):\n",
    "\t\t\t\"\"\"Build NRMS model and scorer.\n",
    "\n",
    "\t\t\tReturns:\n",
    "\t\t\t\t\tobject: a model used to train.\n",
    "\t\t\t\t\tobject: a model used to evaluate and inference.\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tmodel, scorer = self._build_nrms()\n",
    "\t\t\treturn model, scorer\n",
    "\n",
    "\tdef _build_userencoder(self, titleencoder):\n",
    "\t\t\t\"\"\"The main function to create user encoder of NRMS.\n",
    "\n",
    "\t\t\tArgs:\n",
    "\t\t\t\t\ttitleencoder (object): the news encoder of NRMS.\n",
    "\n",
    "\t\t\tReturn:\n",
    "\t\t\t\t\tobject: the user encoder of NRMS.\n",
    "\t\t\t\"\"\"\n",
    "\t\t\this_input_title = tf.keras.Input(\n",
    "\t\t\t\tshape=(self.hparams.history_size, self.hparams.title_size), dtype=\"int32\"\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\tclick_title_presents = tf.keras.layers.TimeDistributed(titleencoder)(\n",
    "\t\t\t\this_input_title\n",
    "\t\t\t)\n",
    "\t\t\ty = SelfAttention(self.hparams.head_num, self.hparams.head_dim, seed=self.seed)(\n",
    "\t\t\t\t[click_title_presents] * 3\n",
    "\t\t\t)\n",
    "\t\t\tuser_present = AttLayer2(self.hparams.attention_hidden_dim, seed=self.seed)(y)\n",
    "\n",
    "\t\t\tmodel = tf.keras.Model(his_input_title, user_present, name=\"user_encoder\")\n",
    "\t\t\treturn model\n",
    "\n",
    "\tdef _build_newsencoder(self):\n",
    "\t\t\t\"\"\"The main function to create news encoder of NRMS.\n",
    "\n",
    "\t\t\tArgs:\n",
    "\t\t\t\t\tembedding_layer (object): a word embedding layer.\n",
    "\n",
    "\t\t\tReturn:\n",
    "\t\t\t\t\tobject: the news encoder of NRMS.\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tembedding_layer = tf.keras.layers.Embedding(\n",
    "\t\t\t\t\tself.word2vec_embedding.shape[0],\n",
    "\t\t\t\t\tself.word2vec_embedding.shape[1],\n",
    "\t\t\t\t\tweights=[self.word2vec_embedding],\n",
    "\t\t\t\t\ttrainable=True,\n",
    "\t\t\t)\n",
    "\t\t\tsequences_input_title = tf.keras.Input(\n",
    "\t\t\t\t\tshape=(self.hparams.title_size,), dtype=\"int32\"\n",
    "\t\t\t)\n",
    "\t\t\tembedded_sequences_title = embedding_layer(sequences_input_title)\n",
    "\n",
    "\t\t\ty = tf.keras.layers.Dropout(self.hparams.dropout)(embedded_sequences_title)\n",
    "\t\t\ty = SelfAttention(self.hparams.head_num, self.hparams.head_dim, seed=self.seed)(\n",
    "\t\t\t\t\t[y, y, y]\n",
    "\t\t\t)\n",
    "\n",
    "\t\t\t# Create configurable Dense layers:\n",
    "\t\t\tfor layer in [400, 400, 400]:\n",
    "\t\t\t\t\ty = tf.keras.layers.Dense(units=layer, activation=\"relu\")(y)\n",
    "\t\t\t\t\ty = tf.keras.layers.BatchNormalization()(y)\n",
    "\t\t\t\t\ty = tf.keras.layers.Dropout(self.hparams.dropout)(y)\n",
    "\n",
    "\t\t\ty = tf.keras.layers.Dropout(self.hparams.dropout)(y)\n",
    "\t\t\tpred_title = AttLayer2(self.hparams.attention_hidden_dim, seed=self.seed)(y)\n",
    "\n",
    "\t\t\tmodel = tf.keras.Model(sequences_input_title, pred_title, name=\"news_encoder\")\n",
    "\t\t\treturn model\n",
    "\n",
    "\tdef _build_nrms(self):\n",
    "\t\t\t\"\"\"The main function to create NRMS's logic. The core of NRMS\n",
    "\t\t\tis a user encoder and a news encoder.\n",
    "\n",
    "\t\t\tReturns:\n",
    "\t\t\t\t\tobject: a model used to train.\n",
    "\t\t\t\t\tobject: a model used to evaluate and inference.\n",
    "\t\t\t\"\"\"\n",
    "\n",
    "\t\t\this_input_title = tf.keras.Input(\n",
    "\t\t\t\t\tshape=(self.hparams.history_size, self.hparams.title_size),\n",
    "\t\t\t\t\tdtype=\"int32\",\n",
    "\t\t\t)\n",
    "\t\t\tpred_input_title = tf.keras.Input(\n",
    "\t\t\t\t\tshape=(None, self.hparams.title_size),\n",
    "\t\t\t\t\tdtype=\"int32\",\n",
    "\t\t\t)\n",
    "\t\t\tpred_input_title_one = tf.keras.Input(\n",
    "\t\t\t\t\tshape=(\n",
    "\t\t\t\t\t\t\t1,\n",
    "\t\t\t\t\t\t\tself.hparams.title_size,\n",
    "\t\t\t\t\t),\n",
    "\t\t\t\t\tdtype=\"int32\",\n",
    "\t\t\t)\n",
    "\t\t\tpred_title_one_reshape = tf.keras.layers.Reshape((self.hparams.title_size,))(\n",
    "\t\t\t\t\tpred_input_title_one\n",
    "\t\t\t)\n",
    "\t\t\ttitleencoder = self._build_newsencoder()\n",
    "\t\t\tself.userencoder = self._build_userencoder(titleencoder)\n",
    "\t\t\tself.newsencoder = titleencoder\n",
    "\n",
    "\t\t\tuser_present = self.userencoder(his_input_title)\n",
    "\t\t\tnews_present = tf.keras.layers.TimeDistributed(self.newsencoder)(\n",
    "\t\t\t\t\tpred_input_title\n",
    "\t\t\t)\n",
    "\t\t\tnews_present_one = self.newsencoder(pred_title_one_reshape)\n",
    "\n",
    "\t\t\tpreds = tf.keras.layers.Dot(axes=-1)([news_present, user_present])\n",
    "\t\t\tpreds = tf.keras.layers.Activation(activation=\"softmax\")(preds)\n",
    "\n",
    "\t\t\tpred_one = tf.keras.layers.Dot(axes=-1)([news_present_one, user_present])\n",
    "\t\t\tpred_one = tf.keras.layers.Activation(activation=\"sigmoid\")(pred_one)\n",
    "\n",
    "\t\t\tmodel = tf.keras.Model([his_input_title, pred_input_title], preds)\n",
    "\t\t\tscorer = tf.keras.Model([his_input_title, pred_input_title_one], pred_one)\n",
    "\n",
    "\t\t\treturn model, scorer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinmoore/anaconda3/envs/eb-nerd/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 2013\n",
      "Validation samples: 329\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>impression_id</th><th>impression_time</th><th>article_id_fixed</th><th>article_ids_clicked</th><th>article_ids_inview</th><th>labels</th></tr><tr><td>u32</td><td>u32</td><td>datetime[μs]</td><td>list[i32]</td><td>list[i64]</td><td>list[i64]</td><td>list[i8]</td></tr></thead><tbody><tr><td>373648</td><td>262207791</td><td>2023-05-20 19:45:35</td><td>[9765551, 9765153, … 9759681]</td><td>[9773744]</td><td>[9268234, 9771009, … 9773726]</td><td>[0, 0, … 0]</td></tr><tr><td>102231</td><td>179933344</td><td>2023-05-22 07:43:11</td><td>[9766889, 9762135, … 9769356]</td><td>[9774595]</td><td>[9772706, 9774595, … 9772706]</td><td>[0, 1, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 7)\n",
       "┌─────────┬──────────────┬──────────────┬──────────────┬──────────────┬──────────────┬─────────────┐\n",
       "│ user_id ┆ impression_i ┆ impression_t ┆ article_id_f ┆ article_ids_ ┆ article_ids_ ┆ labels      │\n",
       "│ ---     ┆ d            ┆ ime          ┆ ixed         ┆ clicked      ┆ inview       ┆ ---         │\n",
       "│ u32     ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ list[i8]    │\n",
       "│         ┆ u32          ┆ datetime[μs] ┆ list[i32]    ┆ list[i64]    ┆ list[i64]    ┆             │\n",
       "╞═════════╪══════════════╪══════════════╪══════════════╪══════════════╪══════════════╪═════════════╡\n",
       "│ 373648  ┆ 262207791    ┆ 2023-05-20   ┆ [9765551,    ┆ [9773744]    ┆ [9268234,    ┆ [0, 0, … 0] │\n",
       "│         ┆              ┆ 19:45:35     ┆ 9765153, …   ┆              ┆ 9771009, …   ┆             │\n",
       "│         ┆              ┆              ┆ 9759681]     ┆              ┆ 9773726]     ┆             │\n",
       "│ 102231  ┆ 179933344    ┆ 2023-05-22   ┆ [9766889,    ┆ [9774595]    ┆ [9772706,    ┆ [0, 1, … 0] │\n",
       "│         ┆              ┆ 07:43:11     ┆ 9762135, …   ┆              ┆ 9774595, …   ┆             │\n",
       "│         ┆              ┆              ┆ 9769356]     ┆              ┆ 9772706]     ┆             │\n",
       "└─────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴─────────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ebrec.utils._behaviors import ebnerd_from_path, create_binary_labels_column, sampling_strategy_wu2019\n",
    "from ebrec.utils._constants import *\n",
    "from ebrec.models.newsrec.model_config import (\n",
    "    hparams_nrms,\n",
    ")\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers, create_article_id_to_value_mapping\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "from ebrec.utils._polars import concat_str_columns\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    " \n",
    "\n",
    "PATH = Path(\"~/Git Repositories/ebnerd-benchmark/data\").expanduser()\n",
    "#\n",
    "DATASPLIT = \"ebnerd_small\"\n",
    "DUMP_DIR = Path(\"ebnerd_predictions\")\n",
    "DUMP_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "HISTORY_SIZE = 20\n",
    "hparams_nrms.history_size = HISTORY_SIZE\n",
    "\n",
    "# Load the necessary columns\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "]\n",
    "# Down sample the number of samples to just run quickly through it.\n",
    "FRACTION = 0.01\n",
    "\n",
    "df = (\n",
    "    ebnerd_from_path(\n",
    "        PATH.joinpath(DATASPLIT, \"train\"),\n",
    "        history_size=HISTORY_SIZE,\n",
    "        padding=0,\n",
    "    )\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=4,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "\n",
    "dt_split = pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL).max() - datetime.timedelta(days=1)\n",
    "df_train = df.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL) < dt_split)\n",
    "df_validation = df.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL) >= dt_split)\n",
    "\n",
    "df_articles = pl.read_parquet(PATH.joinpath(\"articles.parquet\"))\n",
    "\n",
    "print(f\"Train samples: {df_train.height}\\nValidation samples: {df_validation.height}\")\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the same transformer model & tokenizer as in nrms_ebnerd\n",
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "MAX_TITLE_LENGTH = 30\n",
    "TEXT_COLUMNS_TO_USE = [\"subtitle\", \"title\"]\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "\n",
    "# Concatenate text and convert to tokens exactly as the original code\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ebrec.models.newsrec.dataloader import NRMSDataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "val_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# List all physical devices\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(\"Available devices:\", physical_devices)\n",
    "## Iniating the model\n",
    "model = NRMSModel(\n",
    "    hparams=hparams_nrms,\n",
    "    word2vec_embedding=word2vec_embedding,\n",
    "    seed=42,\n",
    ")\n",
    "model.model.compile(\n",
    "    optimizer=model.model.optimizer,\n",
    "    loss=model.model.loss,\n",
    "    metrics=[\"AUC\"],\n",
    ")\n",
    "\n",
    "MODEL_NAME = model.__class__.__name__\n",
    "MODEL_WEIGHTS = DUMP_DIR.joinpath(f\"state_dict/{MODEL_NAME}/weights\")\n",
    "LOG_DIR = DUMP_DIR.joinpath(f\"runs/{MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard:\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=LOG_DIR,\n",
    "    histogram_freq=1,\n",
    ")\n",
    "\n",
    "# Earlystopping:\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_auc\",\n",
    "    mode=\"max\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# ModelCheckpoint:\n",
    "modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=MODEL_WEIGHTS,\n",
    "    monitor=\"val_auc\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Learning rate scheduler:\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_auc\",\n",
    "    mode=\"max\",\n",
    "    factor=0.2,\n",
    "    patience=2,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "callbacks = [tensorboard_callback, early_stopping, modelcheckpoint, lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - ETA: 0s - loss: 3.3325 - auc: 0.5080\n",
      "Epoch 1: val_auc improved from -inf to 0.53097, saving model to ebnerd_predictions/state_dict/NRMSModel/weights\n",
      "63/63 [==============================] - 177s 3s/step - loss: 3.3325 - auc: 0.5080 - val_loss: 1.8427 - val_auc: 0.5310 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "USE_CALLBACKS = True\n",
    "EPOCHS = 1\n",
    "\n",
    "hist = model.model.fit(\n",
    "    train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks if USE_CALLBACKS else [],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CALLBACKS:\n",
    "    _ = model.model.load_weights(filepath=MODEL_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153/153 [==============================] - 127s 830ms/step\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE_TEST = 16\n",
    "df_test = (\n",
    "    ebnerd_from_path(\n",
    "        PATH.joinpath(DATASPLIT, \"validation\"),\n",
    "        history_size=HISTORY_SIZE,\n",
    "        padding=0,\n",
    "    )\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "\n",
    "test_dataloader = NRMSDataLoader(\n",
    "    behaviors=df_test,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=BATCH_SIZE_TEST,\n",
    ")\n",
    "pred_test = model.scorer.predict(test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>impression_id</th><th>impression_time</th><th>article_id_fixed</th><th>article_ids_clicked</th><th>article_ids_inview</th><th>labels</th><th>scores</th></tr><tr><td>u32</td><td>u32</td><td>datetime[μs]</td><td>list[i32]</td><td>list[i32]</td><td>list[i32]</td><td>list[i8]</td><td>list[f64]</td></tr></thead><tbody><tr><td>2451066</td><td>100331502</td><td>2023-05-27 06:17:50</td><td>[9778381, 9778158, … 9779498]</td><td>[9782517]</td><td>[9780467, 9551777, … 9782836]</td><td>[0, 0, … 0]</td><td>[0.557074, 0.327789, … 0.347438]</td></tr><tr><td>2587019</td><td>72206515</td><td>2023-05-28 12:49:15</td><td>[9778628, 9778718, … 9780195]</td><td>[9782407]</td><td>[9782407, 9784947, … 9785145]</td><td>[1, 0, … 0]</td><td>[0.519478, 0.450291, … 0.230027]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 8)\n",
       "┌─────────┬────────────┬────────────┬────────────┬────────────┬────────────┬───────────┬───────────┐\n",
       "│ user_id ┆ impression ┆ impression ┆ article_id ┆ article_id ┆ article_id ┆ labels    ┆ scores    │\n",
       "│ ---     ┆ _id        ┆ _time      ┆ _fixed     ┆ s_clicked  ┆ s_inview   ┆ ---       ┆ ---       │\n",
       "│ u32     ┆ ---        ┆ ---        ┆ ---        ┆ ---        ┆ ---        ┆ list[i8]  ┆ list[f64] │\n",
       "│         ┆ u32        ┆ datetime[μ ┆ list[i32]  ┆ list[i32]  ┆ list[i32]  ┆           ┆           │\n",
       "│         ┆            ┆ s]         ┆            ┆            ┆            ┆           ┆           │\n",
       "╞═════════╪════════════╪════════════╪════════════╪════════════╪════════════╪═══════════╪═══════════╡\n",
       "│ 2451066 ┆ 100331502  ┆ 2023-05-27 ┆ [9778381,  ┆ [9782517]  ┆ [9780467,  ┆ [0, 0, …  ┆ [0.557074 │\n",
       "│         ┆            ┆ 06:17:50   ┆ 9778158, … ┆            ┆ 9551777, … ┆ 0]        ┆ ,         │\n",
       "│         ┆            ┆            ┆ 9779498]   ┆            ┆ 9782836]   ┆           ┆ 0.327789, │\n",
       "│         ┆            ┆            ┆            ┆            ┆            ┆           ┆ …         │\n",
       "│         ┆            ┆            ┆            ┆            ┆            ┆           ┆ 0.347438] │\n",
       "│ 2587019 ┆ 72206515   ┆ 2023-05-28 ┆ [9778628,  ┆ [9782407]  ┆ [9782407,  ┆ [1, 0, …  ┆ [0.519478 │\n",
       "│         ┆            ┆ 12:49:15   ┆ 9778718, … ┆            ┆ 9784947, … ┆ 0]        ┆ ,         │\n",
       "│         ┆            ┆            ┆ 9780195]   ┆            ┆ 9785145]   ┆           ┆ 0.450291, │\n",
       "│         ┆            ┆            ┆            ┆            ┆            ┆           ┆ …         │\n",
       "│         ┆            ┆            ┆            ┆            ┆            ┆           ┆ 0.230027] │\n",
       "└─────────┴────────────┴────────────┴────────────┴────────────┴────────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ebrec.utils._behaviors import add_prediction_scores\n",
    "df_test = add_prediction_scores(df_test, pred_test.tolist())\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AUC:   0%|                                             | 0/2446 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "AUC: 100%|████████████████████████████████| 2446/2446 [00:00<00:00, 2975.87it/s]\n",
      "AUC: 100%|██████████████████████████████| 2446/2446 [00:00<00:00, 107120.67it/s]\n",
      "AUC: 100%|███████████████████████████████| 2446/2446 [00:00<00:00, 52764.51it/s]\n",
      "AUC: 100%|███████████████████████████████| 2446/2446 [00:00<00:00, 51981.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MetricEvaluator class>: \n",
       " {\n",
       "    \"auc\": 0.5066171159200328,\n",
       "    \"mrr\": 0.3136612833230085,\n",
       "    \"ndcg@5\": 0.34801284948037986,\n",
       "    \"ndcg@10\": 0.431464585947497\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "metrics = MetricEvaluator(\n",
    "    labels=df_test[\"labels\"].to_list(),\n",
    "    predictions=df_test[\"scores\"].to_list(),\n",
    "    metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    ")\n",
    "metrics.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2446it [00:00, 3317.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping ebnerd_predictions/predictions.txt to ebnerd_predictions/ebnerd_small_predictions-NRMSModel.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ebrec.utils._python import write_submission_file, rank_predictions_by_score\n",
    "\n",
    "df_test = df_test.with_columns(\n",
    "    pl.col(\"scores\")\n",
    "    .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "    .alias(\"ranked_scores\")\n",
    ")\n",
    "df_test.head(2)\n",
    "\n",
    "write_submission_file(\n",
    "    impression_ids=df_test[DEFAULT_IMPRESSION_ID_COL],\n",
    "    prediction_scores=df_test[\"ranked_scores\"],\n",
    "    path=DUMP_DIR.joinpath(\"predictions.txt\"),\n",
    "    filename_zip=f\"{DATASPLIT}_predictions-{MODEL_NAME}.zip\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eb-nerd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
